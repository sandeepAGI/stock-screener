#!/usr/bin/env python3
"""
Stock Outlier Analytics Dashboard - Enhanced Version with Individual Analysis
Single-file Streamlit application for stakeholder presentation

Usage: streamlit run analytics_dashboard_enhanced.py
"""

import streamlit as st
import sqlite3
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from datetime import datetime
from typing import Optional, Dict, List, Tuple
import sys
import os
import time

# Add project root to path for data collection imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Page configuration
st.set_page_config(
    page_title="Stock Outlier Analysis",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for professional styling with branding
st.markdown("""
<style>
    @import url('https://fonts.googleapis.com/css2?family=Montserrat:wght@300;400;500;600;700&display=swap');

    /* Global font application */
    .main, .sidebar .sidebar-content, .stSelectbox, .stSlider, .stButton,
    .stTextInput, .stMetric, .stMarkdown, h1, h2, h3, h4, h5, h6, p, div {
        font-family: 'Montserrat', sans-serif !important;
    }

    /* Header styling with brand color */
    .main-header {
        color: #60B5E5 !important;
        font-family: 'Montserrat', sans-serif !important;
        font-weight: 700 !important;
        font-size: 2.5rem !important;
        text-align: center;
        margin-bottom: 1rem;
    }

    .metric-container {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        text-align: center;
        font-family: 'Montserrat', sans-serif !important;
    }
    .undervalued {
        border-left: 4px solid #00ff00;
        background-color: #f0fff0;
        padding: 0.5rem;
        margin: 0.25rem 0;
        font-family: 'Montserrat', sans-serif !important;
    }
    .overvalued {
        border-left: 4px solid #ff0000;
        background-color: #fff0f0;
        padding: 0.5rem;
        margin: 0.25rem 0;
        font-family: 'Montserrat', sans-serif !important;
    }
    .stock-details {
        background-color: #f8f9fa;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
        font-family: 'Montserrat', sans-serif !important;
    }

    /* Individual stock analysis styling */
    .metric-card {
        background: #f8f9fa;
        border: 1px solid #e9ecef;
        border-radius: 10px;
        padding: 1rem;
        margin: 0.5rem 0;
        font-family: 'Montserrat', sans-serif !important;
    }
    .score-excellent { color: #28a745; font-weight: bold; font-family: 'Montserrat', sans-serif !important; }
    .score-good { color: #17a2b8; font-weight: bold; font-family: 'Montserrat', sans-serif !important; }
    .score-average { color: #ffc107; font-weight: bold; font-family: 'Montserrat', sans-serif !important; }
    .score-poor { color: #fd7e14; font-weight: bold; font-family: 'Montserrat', sans-serif !important; }
    .score-very-poor { color: #dc3545; font-weight: bold; font-family: 'Montserrat', sans-serif !important; }
    .data-quality-high { color: #28a745; font-family: 'Montserrat', sans-serif !important; }
    .data-quality-medium { color: #ffc107; font-family: 'Montserrat', sans-serif !important; }
    .data-quality-low { color: #dc3545; font-family: 'Montserrat', sans-serif !important; }

    /* Logo container */
    .logo-container {
        display: flex;
        justify-content: center;
        align-items: center;
        margin-bottom: 1rem;
    }

    .logo-container img {
        max-height: 80px;
        width: auto;
    }
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_stock_data() -> pd.DataFrame:
    """Load all stock data with calculated metrics from database."""
    try:
        conn = sqlite3.connect('data/stock_data.db')

        query = """
        SELECT
            cm.symbol,
            s.company_name,
            s.sector,
            s.market_cap,
            cm.composite_score,
            cm.fundamental_score,
            cm.quality_score,
            cm.growth_score,
            cm.sentiment_score,
            cm.sector_percentile,
            cm.calculation_date,
            cm.methodology_version
        FROM calculated_metrics cm
        JOIN stocks s ON cm.symbol = s.symbol
        WHERE cm.composite_score IS NOT NULL
        AND cm.created_at = (
            SELECT MAX(created_at) FROM calculated_metrics cm2
            WHERE cm2.symbol = cm.symbol
        )
        ORDER BY cm.composite_score DESC
        """

        df = pd.read_sql_query(query, conn)
        conn.close()

        # Add derived columns for analysis
        df['market_cap_billions'] = df['market_cap'] / 1e9
        df['composite_rank'] = df['composite_score'].rank(ascending=False, method='min')

        return df

    except Exception as e:
        st.error(f"Error loading data: {e}")
        return pd.DataFrame()

@st.cache_data
def load_sentiment_data(symbol: str) -> pd.DataFrame:
    """Load sentiment details for a specific stock."""
    try:
        conn = sqlite3.connect('data/stock_data.db')

        # Get recent news headlines (prefer articles with titles, deduplicated)
        news_query = """
        SELECT DISTINCT title, publish_date, sentiment_score, url
        FROM news_articles
        WHERE symbol = ?
        ORDER BY
            CASE WHEN title IS NOT NULL AND LENGTH(title) > 0 THEN 0 ELSE 1 END,
            publish_date DESC
        LIMIT 5
        """

        news_df = pd.read_sql_query(news_query, conn, params=[symbol])
        conn.close()

        return news_df

    except Exception as e:
        st.error(f"Error loading sentiment data: {e}")
        return pd.DataFrame()

def calculate_custom_composite_scores(df: pd.DataFrame, weights: List[float]) -> pd.DataFrame:
    """Recalculate composite scores with custom weights."""
    df_copy = df.copy()

    # Normalize weights to sum to 1
    weights = np.array(weights)
    weights = weights / weights.sum()

    # Calculate new composite scores
    df_copy['custom_composite_score'] = (
        df_copy['fundamental_score'] * weights[0] +
        df_copy['quality_score'] * weights[1] +
        df_copy['growth_score'] * weights[2] +
        df_copy['sentiment_score'] * weights[3]
    )

    # Calculate new rankings
    df_copy['custom_composite_rank'] = df_copy['custom_composite_score'].rank(ascending=False, method='min')
    df_copy['rank_change'] = df_copy['composite_rank'] - df_copy['custom_composite_rank']

    return df_copy.sort_values('custom_composite_score', ascending=False)

def get_database_stats() -> Dict:
    """Get basic database statistics for the summary."""
    try:
        conn = sqlite3.connect('data/stock_data.db')

        stats = {}

        # Count total stocks
        stats['total_stocks'] = pd.read_sql_query(
            "SELECT COUNT(*) as count FROM stocks", conn
        ).iloc[0]['count']

        # Count stocks with calculations (latest only)
        stats['calculated_stocks'] = pd.read_sql_query(
            """SELECT COUNT(DISTINCT symbol) as count
               FROM calculated_metrics
               WHERE composite_score IS NOT NULL""", conn
        ).iloc[0]['count']

        # Calculate coverage percentage
        if stats['total_stocks'] > 0:
            stats['coverage_pct'] = (stats['calculated_stocks'] / stats['total_stocks']) * 100
        else:
            stats['coverage_pct'] = 0

        # Get last calculation date
        last_calc = pd.read_sql_query(
            "SELECT MAX(created_at) as last_calc FROM calculated_metrics", conn
        ).iloc[0]['last_calc']

        if last_calc:
            stats['last_calculation'] = last_calc
        else:
            stats['last_calculation'] = 'Unknown'

        conn.close()
        return stats

    except Exception as e:
        st.error(f"Error getting database stats: {e}")
        return {'total_stocks': 0, 'calculated_stocks': 0, 'coverage_pct': 0, 'last_calculation': 'Unknown'}

def show_weight_adjusted_rankings(original_df: pd.DataFrame, adjusted_df: pd.DataFrame, weights: List[float]) -> None:
    """Display side-by-side comparison of original vs adjusted rankings."""

    weight_labels = ['Fundamental', 'Quality', 'Growth', 'Sentiment']
    weights_normalized = np.array(weights) / np.sum(weights)

    # Display current weights
    cols = st.columns(4)
    for i, (label, weight) in enumerate(zip(weight_labels, weights_normalized)):
        with cols[i]:
            st.metric(f"{label}", f"{weight:.1%}")

    # Side-by-side rankings
    col_orig, col_adj = st.columns(2)

    with col_orig:
        st.markdown("**ðŸ† Original Rankings (40/25/20/15)**")
        top_orig = original_df.head(10)[['symbol', 'company_name', 'composite_score']].copy()
        top_orig.index = range(1, len(top_orig) + 1)
        st.dataframe(top_orig, use_container_width=True)

    with col_adj:
        st.markdown("**ðŸŽ¯ Adjusted Rankings (Custom Weights)**")
        top_adj = adjusted_df.head(10)[['symbol', 'company_name', 'custom_composite_score']].copy()
        top_adj.columns = ['symbol', 'company_name', 'composite_score']
        top_adj.index = range(1, len(top_adj) + 1)
        st.dataframe(top_adj, use_container_width=True)

    # Show biggest movers
    st.subheader("ðŸ“Š Biggest Ranking Changes")

    # Filter for significant moves
    big_movers = adjusted_df[abs(adjusted_df['rank_change']) >= 5].copy()

    if len(big_movers) > 0:
        col_up, col_down = st.columns(2)

        with col_up:
            st.markdown("**ðŸ“ˆ Biggest Gainers**")
            gainers = big_movers[big_movers['rank_change'] > 0].nlargest(5, 'rank_change')
            if len(gainers) > 0:
                for _, stock in gainers.iterrows():
                    st.success(f"ðŸ“ˆ **{stock['symbol']}** moved up {int(stock['rank_change'])} positions")
            else:
                st.info("No significant gainers with current weight adjustment")

        with col_down:
            st.markdown("**ðŸ“‰ Biggest Losers**")
            losers = big_movers[big_movers['rank_change'] < 0].nsmallest(5, 'rank_change')
            if len(losers) > 0:
                for _, stock in losers.iterrows():
                    st.error(f"ðŸ“‰ **{stock['symbol']}** moved down {int(abs(stock['rank_change']))} positions")
            else:
                st.info("No significant losers with current weight adjustment")
    else:
        st.info("No significant ranking changes (Â±5 positions) with current weight adjustment")

def show_top_performers(df: pd.DataFrame, top_n: int = 5, ascending: bool = False, score_column: str = 'composite_score') -> None:
    """Display top N stocks in a formatted table."""
    if ascending:
        top_stocks = df.nsmallest(top_n, score_column)
        container_class = "overvalued"
        title = f"ðŸ”´ Most Overvalued (Bottom {top_n})"
    else:
        top_stocks = df.nlargest(top_n, score_column)
        container_class = "undervalued"
        title = f"ðŸŸ¢ Most Undervalued (Top {top_n})"

    st.markdown(f"**{title}**")

    for _, stock in top_stocks.iterrows():
        with st.container():
            col1, col2, col3, col4 = st.columns([2, 2, 1, 1])

            with col1:
                st.markdown(f"""
                <div class="{container_class}">
                    <strong>{stock['symbol']}</strong><br>
                    <small>{stock['company_name']}</small>
                </div>
                """, unsafe_allow_html=True)

            with col2:
                st.markdown(f"""
                <div class="stock-details">
                    <strong>Sector:</strong> {stock['sector']}<br>
                    <strong>Market Cap:</strong> ${stock['market_cap_billions']:.1f}B
                </div>
                """, unsafe_allow_html=True)

            with col3:
                # Use the appropriate score column
                score_value = stock[score_column] if score_column in stock else stock.get('composite_score', 0)
                st.metric("Score", f"{score_value:.1f}")

            with col4:
                # Use the appropriate rank column
                if score_column == 'custom_composite_score':
                    rank_value = stock.get('custom_composite_rank', stock.get('composite_rank', 0))
                else:
                    rank_value = stock.get('composite_rank', 0)
                st.metric("Rank", f"#{int(rank_value)}")

def show_detailed_analysis(symbol: str, df: pd.DataFrame):
    """Show detailed analysis for a selected stock with sentiment data."""
    try:
        stock_data = df[df['symbol'] == symbol].iloc[0]

        # Stock header
        st.markdown(f"### ðŸ“ˆ {stock_data['company_name']} ({symbol})")
        st.markdown(f"**Sector:** {stock_data['sector']} | **Market Cap:** ${stock_data['market_cap_billions']:.1f}B")

        # Key metrics in columns
        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric(
                "Composite Score",
                f"{stock_data['composite_score']:.1f}",
                help="Overall weighted score based on 4 components"
            )

        with col2:
            sector_pct = stock_data['sector_percentile']
            if sector_pct is not None and not pd.isna(sector_pct):
                st.metric("Sector Percentile", f"{sector_pct:.1f}%")
            else:
                st.metric("Market Rank", f"#{int(stock_data['composite_rank'])}")

        with col3:
            market_cap_rank = "Large" if stock_data['market_cap_billions'] > 10 else "Mid" if stock_data['market_cap_billions'] > 2 else "Small"
            st.metric("Cap Category", market_cap_rank)

        # Component scores breakdown
        st.subheader("ðŸ“Š Component Analysis")

        # Create visual breakdown
        comp_col1, comp_col2, comp_col3, comp_col4 = st.columns(4)

        with comp_col1:
            st.metric("Fundamental (40%)", f"{stock_data['fundamental_score']:.0f}/100")
        with comp_col2:
            st.metric("Quality (25%)", f"{stock_data['quality_score']:.0f}/100")
        with comp_col3:
            st.metric("Growth (20%)", f"{stock_data['growth_score']:.0f}/100")
        with comp_col4:
            st.metric("Sentiment (15%)", f"{stock_data['sentiment_score']:.0f}/100")

        # Load and display sentiment data
        sentiment_df = load_sentiment_data(symbol)

        if not sentiment_df.empty and 'title' in sentiment_df.columns:
            # Filter out rows with empty or null titles
            valid_news = sentiment_df[
                (sentiment_df['title'].notna()) &
                (sentiment_df['title'] != '') &
                (sentiment_df['title'] != 'No title available')
            ]

            if len(valid_news) > 0:
                st.subheader("ðŸ“° Recent News Analysis")

                for _, news in valid_news.head(3).iterrows():
                    sentiment_emoji = "ðŸŸ¢" if news['sentiment_score'] > 0.1 else "ðŸ”´" if news['sentiment_score'] < -0.1 else "âšª"

                    with st.expander(f"{sentiment_emoji} {news['title'][:80]}..."):
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.write(f"**Published:** {news['publish_date']}")
                            if pd.notna(news['url']) and news['url']:
                                st.markdown(f"[Read full article]({news['url']})")
                        with col2:
                            st.metric("Sentiment", f"{news['sentiment_score']:.2f}")
            else:
                st.info("No recent news headlines available (news data collection needs refresh).")
        else:
            st.subheader("Recent News Headlines")
            st.info("No recent news headlines available for this stock.")

    except Exception as e:
        st.error(f"Error displaying detailed analysis: {e}")

# Individual Stock Analysis Functions
def get_score_class(score: float) -> str:
    """Get CSS class for score styling"""
    if score >= 80:
        return "score-excellent"
    elif score >= 70:
        return "score-good"
    elif score >= 50:
        return "score-average"
    elif score >= 30:
        return "score-poor"
    else:
        return "score-very-poor"

def get_data_quality_class(quality: float) -> str:
    """Get CSS class for data quality styling"""
    if quality >= 0.8:
        return "data-quality-high"
    elif quality >= 0.6:
        return "data-quality-medium"
    else:
        return "data-quality-low"

def format_score(score: float) -> str:
    """Format score with appropriate styling"""
    css_class = get_score_class(score)
    return f'<span class="{css_class}">{score:.1f}</span>'

def format_data_quality(quality: float) -> str:
    """Format data quality with appropriate styling"""
    css_class = get_data_quality_class(quality)
    percentage = quality * 100
    return f'<span class="{css_class}">{percentage:.0f}%</span>'

def show_individual_stock_analysis(df: pd.DataFrame):
    """Display enhanced individual stock analysis with underlying metrics and trends"""
    st.header("ðŸ“ˆ Individual Stock Analysis")

    if df.empty:
        st.error("No data available for individual stock analysis.")
        return

    # Stock selection
    col1, col2 = st.columns([2, 1])

    with col1:
        df_sorted = df.sort_values('composite_score', ascending=False)
        stock_options = {}
        for _, row in df_sorted.iterrows():
            display_name = f"{row['symbol']} - {row['company_name']} (Score: {row['composite_score']:.1f})"
            stock_options[display_name] = row['symbol']

        st.markdown("**ðŸ’¡ Search Tips:** Type ticker symbol or company name to quickly find stocks")

        selected_display = st.selectbox(
            "ðŸ” Select a stock to analyze:",
            options=list(stock_options.keys()),
            help="Start typing to search. Sorted by composite score.",
            key="stock_selector"
        )
        selected_symbol = stock_options[selected_display]

    with col2:
        st.metric("Total Stocks", len(df))
        selected_rank = df[df['symbol'] == selected_symbol]['composite_rank'].iloc[0]
        st.metric("Selected Rank", f"#{int(selected_rank)}")

    # Get selected stock data
    stock_data = df[df['symbol'] == selected_symbol].iloc[0]

    # Fetch additional data from database
    from src.data.database import DatabaseManager
    db = DatabaseManager()

    fundamental_history = []
    news_articles = []
    reddit_posts = []
    peer_data = []

    if db.connect():
        cursor = db.connection.cursor()

        # Fetch fundamental history
        cursor.execute("""
            SELECT reporting_date, pe_ratio, forward_pe, peg_ratio, price_to_book,
                   eps, total_revenue, net_income, free_cash_flow,
                   total_debt, shareholders_equity, return_on_equity, debt_to_equity, current_ratio,
                   revenue_growth, earnings_growth
            FROM fundamental_data
            WHERE symbol = ?
            ORDER BY reporting_date DESC
            LIMIT 15
        """, (selected_symbol,))
        fundamental_history = cursor.fetchall()

        # Fetch news articles
        cursor.execute("""
            SELECT title, publish_date, sentiment_score, data_quality_score, url, publisher
            FROM news_articles
            WHERE symbol = ?
            ORDER BY publish_date DESC
            LIMIT 50
        """, (selected_symbol,))
        news_articles = cursor.fetchall()

        # Fetch reddit posts
        cursor.execute("""
            SELECT title, created_at, sentiment_score, subreddit, score
            FROM reddit_posts
            WHERE symbol = ?
            ORDER BY created_at DESC
            LIMIT 50
        """, (selected_symbol,))
        reddit_posts = cursor.fetchall()

        # Fetch industry peers (direct competitors)
        industry_peers = []
        if 'industry' in stock_data and pd.notna(stock_data['industry']):
            cursor.execute("""
                SELECT s.symbol, s.company_name, cm.composite_score,
                       cm.fundamental_score, cm.quality_score, cm.growth_score
                FROM stocks s
                JOIN calculated_metrics cm ON s.symbol = cm.symbol
                WHERE s.industry = ? AND s.symbol != ? AND s.is_active = 1
                AND cm.calculation_date = (
                    SELECT MAX(calculation_date) FROM calculated_metrics WHERE symbol = s.symbol
                )
                ORDER BY cm.composite_score DESC
                LIMIT 5
            """, (stock_data['industry'], selected_symbol))
            industry_peers = cursor.fetchall()

        # Fetch sector peers (broader context, top performers)
        sector_peers = []
        if 'sector' in stock_data and pd.notna(stock_data['sector']):
            cursor.execute("""
                SELECT s.symbol, s.company_name, cm.composite_score,
                       cm.fundamental_score, cm.quality_score, cm.growth_score
                FROM stocks s
                JOIN calculated_metrics cm ON s.symbol = cm.symbol
                WHERE s.sector = ? AND s.symbol != ? AND s.is_active = 1
                AND cm.calculation_date = (
                    SELECT MAX(calculation_date) FROM calculated_metrics WHERE symbol = s.symbol
                )
                ORDER BY cm.composite_score DESC
                LIMIT 5
            """, (stock_data['sector'], selected_symbol))
            sector_peers = cursor.fetchall()

        cursor.close()
        db.close()

    # === SECTION 1: STOCK HEADER & KEY METRICS ===
    st.subheader(f"ðŸ“Š {stock_data['company_name']} ({selected_symbol})")

    # Display sector and industry
    caption_parts = []
    if 'industry' in stock_data and pd.notna(stock_data['industry']):
        caption_parts.append(f"Industry: {stock_data['industry']}")
    if 'sector' in stock_data and pd.notna(stock_data['sector']):
        caption_parts.append(f"Sector: {stock_data['sector']}")
    if caption_parts:
        st.caption(" | ".join(caption_parts))

    # Calculate actual data quality
    overall_quality = 0.0
    quality_components = []

    if news_articles:
        news_quality = sum([row[3] if row[3] else 0 for row in news_articles]) / len(news_articles)
        quality_components.append(news_quality)

    if fundamental_history:
        quality_components.append(0.9)  # Fundamental data is generally high quality

    if quality_components:
        overall_quality = sum(quality_components) / len(quality_components)

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("Composite Score", f"{stock_data['composite_score']:.1f}", help="Overall weighted score (0-100)")

    with col2:
        if 'sector_percentile' in stock_data and pd.notna(stock_data['sector_percentile']):
            st.metric("Sector Percentile", f"{stock_data['sector_percentile']:.1f}%", help="Ranking within sector")
        else:
            st.metric("Market Rank", f"#{int(stock_data['composite_rank'])}")

    with col3:
        st.metric("Data Quality", f"{overall_quality*100:.0f}%", help="Based on news quality and data completeness")

    with col4:
        score = stock_data['composite_score']
        category = "Undervalued" if score >= 70 else ("Balanced" if score >= 50 else "Overvalued")
        st.metric("Category", category)

    # === SECTION 2: COMPONENT SCORE OVERVIEW ===
    st.markdown("---")
    st.subheader("ðŸ“Š Component Score Overview")

    categories = ['Fundamental\\n(40%)', 'Quality\\n(25%)', 'Growth\\n(20%)', 'Sentiment\\n(15%)']
    scores = [
        stock_data['fundamental_score'],
        stock_data['quality_score'],
        stock_data['growth_score'],
        stock_data['sentiment_score']
    ]

    fig = go.Figure()
    fig.add_trace(go.Scatterpolar(
        r=scores,
        theta=categories,
        fill='toself',
        name='Component Scores',
        line_color='rgb(96, 181, 229)',
        fillcolor='rgba(96, 181, 229, 0.3)'
    ))

    fig.update_layout(
        polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
        showlegend=False,
        title="Component Score Distribution",
        height=350,
        font=dict(family='Montserrat')
    )

    st.plotly_chart(fig, use_container_width=True)

    # === SECTION 3: UNDERLYING METRICS BREAKDOWN ===
    st.markdown("---")
    st.subheader("ðŸ“ˆ Underlying Metrics Breakdown")

    # Get most recent and previous fundamentals
    current_fundamentals = fundamental_history[0] if len(fundamental_history) > 0 else None
    previous_fundamentals = fundamental_history[1] if len(fundamental_history) > 1 else None

    if current_fundamentals:
        current_date = current_fundamentals[0]
        prev_date = previous_fundamentals[0] if previous_fundamentals else None

        # Helper function to calculate change
        def calc_change(current, previous):
            if previous and previous != 0:
                return ((current - previous) / previous) * 100
            return 0.0

        # Fundamental Metrics Expander
        with st.expander("ðŸ“Š Fundamental Metrics (40% weight)", expanded=True):
            if current_fundamentals:
                st.markdown(f"**ðŸ“… Latest Data:** {current_date}" + (f" | **Comparing to:** {prev_date}" if prev_date else ""))

                metrics_data = {
                    "Metric": ["P/E Ratio", "Forward P/E", "PEG Ratio", "Price/Book"],
                    "Current": [
                        f"{current_fundamentals[1]:.2f}" if current_fundamentals[1] else "N/A",
                        f"{current_fundamentals[2]:.2f}" if current_fundamentals[2] else "N/A",
                        f"{current_fundamentals[3]:.2f}" if current_fundamentals[3] else "N/A",
                        f"{current_fundamentals[4]:.2f}" if current_fundamentals[4] else "N/A",
                    ],
                    "Change": []
                }

                if previous_fundamentals:
                    for i, (curr, prev) in enumerate(zip(current_fundamentals[1:5], previous_fundamentals[1:5])):
                        if curr and prev:
                            change = calc_change(curr, prev)
                            arrow = "â–²" if change > 0 else ("â–¼" if change < 0 else "â”€")
                            metrics_data["Change"].append(f"{arrow} {abs(change):.1f}%")
                        else:
                            metrics_data["Change"].append("N/A")
                else:
                    metrics_data["Change"] = ["N/A"] * 4

                df_metrics = pd.DataFrame(metrics_data)
                st.dataframe(df_metrics, use_container_width=True, hide_index=True)
            else:
                st.info("No fundamental data available")

        # Quality Metrics Expander
        with st.expander("ðŸ† Quality Metrics (25% weight)"):
            if current_fundamentals:
                # Use pre-calculated quality metrics from Yahoo Finance
                roe = current_fundamentals[11]  # return_on_equity
                debt_to_equity = current_fundamentals[12]  # debt_to_equity
                current_ratio = current_fundamentals[13]  # current_ratio

                metrics_data = {
                    "Metric": [],
                    "Current": [],
                    "Change": []
                }

                # ROE
                if roe is not None:
                    roe_pct = roe * 100  # Convert to percentage
                    metrics_data["Metric"].append("ROE (Return on Equity)")
                    metrics_data["Current"].append(f"{roe_pct:.1f}%")

                    if previous_fundamentals and previous_fundamentals[11] is not None:
                        prev_roe = previous_fundamentals[11] * 100
                        change = calc_change(roe_pct, prev_roe)
                        arrow = "â–²" if change > 0 else ("â–¼" if change < 0 else "â”€")
                        metrics_data["Change"].append(f"{arrow} {abs(change):.1f}%")
                    else:
                        metrics_data["Change"].append("â”€")

                # Debt-to-Equity
                if debt_to_equity is not None:
                    metrics_data["Metric"].append("Debt-to-Equity Ratio")
                    metrics_data["Current"].append(f"{debt_to_equity:.2f}")

                    if previous_fundamentals and previous_fundamentals[12] is not None:
                        prev_d2e = previous_fundamentals[12]
                        change = calc_change(debt_to_equity, prev_d2e)
                        arrow = "â–²" if change > 0 else ("â–¼" if change < 0 else "â”€")
                        metrics_data["Change"].append(f"{arrow} {abs(change):.1f}%")
                    else:
                        metrics_data["Change"].append("â”€")

                # Current Ratio
                if current_ratio is not None:
                    metrics_data["Metric"].append("Current Ratio")
                    metrics_data["Current"].append(f"{current_ratio:.2f}")

                    if previous_fundamentals and previous_fundamentals[13] is not None:
                        prev_cr = previous_fundamentals[13]
                        change = calc_change(current_ratio, prev_cr)
                        arrow = "â–²" if change > 0 else ("â–¼" if change < 0 else "â”€")
                        metrics_data["Change"].append(f"{arrow} {abs(change):.1f}%")
                    else:
                        metrics_data["Change"].append("â”€")

                if metrics_data["Metric"]:
                    df_quality = pd.DataFrame(metrics_data)
                    st.dataframe(df_quality, use_container_width=True, hide_index=True)
                else:
                    st.info("No quality metrics available from Yahoo Finance")
            else:
                st.info("No fundamental data available")

        # Growth Metrics Expander
        with st.expander("ðŸ“ˆ Growth Metrics (20% weight)"):
            if current_fundamentals:
                # Use pre-calculated growth metrics from Yahoo Finance
                revenue_growth = current_fundamentals[14]  # revenue_growth
                earnings_growth = current_fundamentals[15]  # earnings_growth

                metrics_data = {
                    "Metric": [],
                    "Current": [],
                    "Change": []
                }

                # Revenue Growth
                if revenue_growth is not None:
                    revenue_growth_pct = revenue_growth * 100  # Convert to percentage
                    metrics_data["Metric"].append("Revenue Growth")
                    metrics_data["Current"].append(f"{revenue_growth_pct:+.1f}%")

                    if previous_fundamentals and previous_fundamentals[14] is not None:
                        prev_rev_growth = previous_fundamentals[14] * 100
                        change = calc_change(revenue_growth_pct, prev_rev_growth)
                        arrow = "â–²" if change > 0 else ("â–¼" if change < 0 else "â”€")
                        metrics_data["Change"].append(f"{arrow} {abs(change):.1f}pp")
                    else:
                        metrics_data["Change"].append("â”€")

                # Earnings Growth
                if earnings_growth is not None:
                    earnings_growth_pct = earnings_growth * 100  # Convert to percentage
                    metrics_data["Metric"].append("EPS Growth")
                    metrics_data["Current"].append(f"{earnings_growth_pct:+.1f}%")

                    if previous_fundamentals and previous_fundamentals[15] is not None:
                        prev_earn_growth = previous_fundamentals[15] * 100
                        change = calc_change(earnings_growth_pct, prev_earn_growth)
                        arrow = "â–²" if change > 0 else ("â–¼" if change < 0 else "â”€")
                        metrics_data["Change"].append(f"{arrow} {abs(change):.1f}pp")
                    else:
                        metrics_data["Change"].append("â”€")

                if metrics_data["Metric"]:
                    df_growth = pd.DataFrame(metrics_data)
                    st.dataframe(df_growth, use_container_width=True, hide_index=True)
                else:
                    st.info("No growth metrics available from Yahoo Finance")
            else:
                st.info("No fundamental data available")

        # Sentiment Metrics Expander
        with st.expander("ðŸ’­ Sentiment Metrics (15% weight)"):
            col1, col2 = st.columns(2)

            with col1:
                if news_articles:
                    news_sentiments = [row[2] for row in news_articles if row[2] is not None]
                    if news_sentiments:
                        avg_news = sum(news_sentiments) / len(news_sentiments)
                        st.metric("News Sentiment Avg", f"{avg_news:.2f}")
                        st.caption(f"Based on {len(news_sentiments)} articles")
                    else:
                        st.info("No news sentiment data")
                else:
                    st.info("No news articles")

            with col2:
                if reddit_posts:
                    reddit_sentiments = [row[2] for row in reddit_posts if row[2] is not None]
                    if reddit_sentiments:
                        avg_reddit = sum(reddit_sentiments) / len(reddit_sentiments)
                        st.metric("Reddit Sentiment Avg", f"{avg_reddit:.2f}")
                        st.caption(f"Based on {len(reddit_sentiments)} posts")
                    else:
                        st.info("No Reddit sentiment data")
                else:
                    st.info("No Reddit posts")
    else:
        st.info("No fundamental data available for this stock")

    # === SECTION 4: HISTORICAL TRENDS ===
    if len(fundamental_history) >= 3:
        st.markdown("---")
        st.subheader(f"ðŸ“ˆ Historical Trends ({len(fundamental_history)} data points)")

        # Create small multiple charts
        col1, col2, col3 = st.columns(3)

        # Prepare data
        dates = [row[0] for row in fundamental_history[::-1]]  # Reverse for chronological
        pe_ratios = [row[1] for row in fundamental_history[::-1]]
        peg_ratios = [row[3] for row in fundamental_history[::-1]]
        composite_history = []

        # Get composite score history
        if db.connect():
            cursor = db.connection.cursor()
            cursor.execute("""
                SELECT calculation_date, composite_score
                FROM calculated_metrics
                WHERE symbol = ?
                ORDER BY calculation_date DESC
                LIMIT 15
            """, (selected_symbol,))
            composite_history = cursor.fetchall()[::-1]  # Reverse for chronological
            cursor.close()
            db.close()

        with col1:
            fig_pe = go.Figure()
            fig_pe.add_trace(go.Scatter(
                x=dates,
                y=pe_ratios,
                mode='lines+markers',
                name='P/E Ratio',
                line=dict(color='rgb(96, 181, 229)', width=2),
                marker=dict(size=6)
            ))
            fig_pe.update_layout(
                title="P/E Ratio Trend",
                height=250,
                margin=dict(l=20, r=20, t=40, b=20),
                font=dict(family='Montserrat', size=10)
            )
            st.plotly_chart(fig_pe, use_container_width=True)

        with col2:
            fig_peg = go.Figure()
            fig_peg.add_trace(go.Scatter(
                x=dates,
                y=peg_ratios,
                mode='lines+markers',
                name='PEG Ratio',
                line=dict(color='rgb(96, 181, 229)', width=2),
                marker=dict(size=6)
            ))
            fig_peg.update_layout(
                title="PEG Ratio Trend",
                height=250,
                margin=dict(l=20, r=20, t=40, b=20),
                font=dict(family='Montserrat', size=10)
            )
            st.plotly_chart(fig_peg, use_container_width=True)

        with col3:
            if composite_history:
                comp_dates = [row[0] for row in composite_history]
                comp_scores = [row[1] for row in composite_history]

                fig_comp = go.Figure()
                fig_comp.add_trace(go.Scatter(
                    x=comp_dates,
                    y=comp_scores,
                    mode='lines+markers',
                    name='Composite Score',
                    line=dict(color='rgb(96, 181, 229)', width=2),
                    marker=dict(size=6)
                ))
                fig_comp.update_layout(
                    title="Composite Score Trend",
                    height=250,
                    margin=dict(l=20, r=20, t=40, b=20),
                    font=dict(family='Montserrat', size=10)
                )
                st.plotly_chart(fig_comp, use_container_width=True)

    # === SECTION 5: RECENT NEWS & SENTIMENT ===
    st.markdown("---")
    st.subheader("ðŸ“° Recent News & Sentiment")

    with st.expander(f"ðŸ“° News Articles ({len(news_articles)} articles)", expanded=False):
        if news_articles:
            # Sentiment distribution
            news_sentiments = [row[2] for row in news_articles if row[2] is not None]
            if news_sentiments:
                positive = len([s for s in news_sentiments if s > 0.3])
                neutral = len([s for s in news_sentiments if -0.3 <= s <= 0.3])
                negative = len([s for s in news_sentiments if s < -0.3])

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ðŸŸ¢ Positive", positive)
                with col2:
                    st.metric("ðŸŸ¡ Neutral", neutral)
                with col3:
                    st.metric("ðŸ”´ Negative", negative)

                st.markdown("---")
                st.markdown("**Latest Headlines:**")

                for i, article in enumerate(news_articles[:10]):
                    title, date, sentiment, quality, url, publisher = article
                    date_str = str(date)[:10] if date else "N/A"
                    if sentiment is not None:
                        emoji = "ðŸŸ¢" if sentiment > 0.3 else ("ðŸŸ¡" if sentiment >= -0.3 else "ðŸ”´")
                        st.markdown(f"{emoji} **{sentiment:.1f}** | {title[:100]}... | *{date_str}*")
                    else:
                        st.markdown(f"âšª **N/A** | {title[:100]}... | *{date_str}*")
        else:
            st.info("No news articles available")

    with st.expander(f"ðŸ’­ Reddit Discussions ({len(reddit_posts)} posts)", expanded=False):
        if reddit_posts:
            # Sentiment distribution
            reddit_sentiments = [row[2] for row in reddit_posts if row[2] is not None]
            if reddit_sentiments:
                positive = len([s for s in reddit_sentiments if s > 0.3])
                neutral = len([s for s in reddit_sentiments if -0.3 <= s <= 0.3])
                negative = len([s for s in reddit_sentiments if s < -0.3])

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ðŸŸ¢ Positive", positive)
                with col2:
                    st.metric("ðŸŸ¡ Neutral", neutral)
                with col3:
                    st.metric("ðŸ”´ Negative", negative)

                st.markdown("---")
                st.markdown("**Recent Discussions:**")

                for post in reddit_posts[:10]:
                    title, date, sentiment, subreddit, score = post
                    date_str = str(date)[:10] if date else "N/A"
                    if sentiment is not None:
                        emoji = "ðŸŸ¢" if sentiment > 0.3 else ("ðŸŸ¡" if sentiment >= -0.3 else "ðŸ”´")
                        st.markdown(f"{emoji} **{sentiment:.1f}** | {title[:100]}... | r/{subreddit} | *{date_str}*")
                    else:
                        st.markdown(f"âšª **N/A** | {title[:100]}... | r/{subreddit} | *{date_str}*")
        else:
            st.info("No Reddit posts available")

    # === SECTION 6: PEER COMPARISON ===
    st.markdown("---")
    st.subheader("ðŸ¢ Peer Comparison")

    # Helper function to display peer comparison
    def display_peer_comparison(peers, comparison_title, comparison_subtitle):
        if not peers:
            return

        st.markdown(f"#### {comparison_title}")
        st.caption(comparison_subtitle)

        # Prepare comparison data
        comparison_data = {
            "Stock": [selected_symbol] + [row[0] for row in peers],
            "Company": [stock_data['company_name'][:20]] + [row[1][:20] for row in peers],
            "Composite": [stock_data['composite_score']] + [row[2] for row in peers],
            "Fundamental": [stock_data['fundamental_score']] + [row[3] for row in peers],
            "Quality": [stock_data['quality_score']] + [row[4] for row in peers],
            "Growth": [stock_data['growth_score']] + [row[5] for row in peers],
        }

        df_peers = pd.DataFrame(comparison_data)

        # Display table
        st.dataframe(
            df_peers.style.highlight_max(subset=['Composite', 'Fundamental', 'Quality', 'Growth'], color='lightgreen'),
            use_container_width=True,
            hide_index=True
        )

        # Bar chart comparison
        fig_peers = go.Figure()

        fig_peers.add_trace(go.Bar(
            name=selected_symbol,
            x=['Composite', 'Fundamental', 'Quality', 'Growth'],
            y=[stock_data['composite_score'], stock_data['fundamental_score'],
               stock_data['quality_score'], stock_data['growth_score']],
            marker_color='rgb(96, 181, 229)'
        ))

        # Average of peers
        avg_composite = sum([row[2] for row in peers]) / len(peers)
        avg_fundamental = sum([row[3] for row in peers]) / len(peers)
        avg_quality = sum([row[4] for row in peers]) / len(peers)
        avg_growth = sum([row[5] for row in peers]) / len(peers)

        fig_peers.add_trace(go.Bar(
            name='Peer Average',
            x=['Composite', 'Fundamental', 'Quality', 'Growth'],
            y=[avg_composite, avg_fundamental, avg_quality, avg_growth],
            marker_color='lightgray'
        ))

        fig_peers.update_layout(
            title=f"{selected_symbol} vs Peer Average",
            barmode='group',
            height=350,
            font=dict(family='Montserrat')
        )

        st.plotly_chart(fig_peers, use_container_width=True)

    # Display Industry Comparison (direct competitors)
    if industry_peers and len(industry_peers) >= 1:
        display_peer_comparison(
            industry_peers,
            f"ðŸŽ¯ Industry Comparison: {stock_data.get('industry', 'Unknown')}",
            f"Direct competitors - {len(industry_peers) + 1} stocks in same industry"
        )

        if sector_peers:
            st.markdown("---")

    # Display Sector Comparison (broader context)
    if sector_peers:
        display_peer_comparison(
            sector_peers,
            f"ðŸ“Š Sector Comparison: {stock_data.get('sector', 'Unknown')}",
            f"Top performers in sector - includes sector-level scoring adjustments"
        )

    # If neither comparison available
    if not industry_peers and not sector_peers:
        st.info("No peer comparison data available")

    # === SECTION 7: INVESTMENT INSIGHTS ===
    st.markdown("---")
    st.subheader("ðŸ’¡ Investment Insights")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**ðŸŽ¯ Strengths**")
        component_scores = [
            ("Fundamental", stock_data['fundamental_score']),
            ("Quality", stock_data['quality_score']),
            ("Growth", stock_data['growth_score']),
            ("Sentiment", stock_data['sentiment_score'])
        ]
        component_scores.sort(key=lambda x: x[1], reverse=True)

        for name, score in component_scores[:2]:
            if score >= 60:
                st.success(f"âœ“ Strong {name} metrics ({score:.1f}/100)")
            else:
                st.info(f"â€¢ {name} showing potential ({score:.1f}/100)")

        # Add trend-based insights
        if len(fundamental_history) >= 2 and current_fundamentals and previous_fundamentals:
            pe_change = calc_change(current_fundamentals[1], previous_fundamentals[1]) if current_fundamentals[1] and previous_fundamentals[1] else 0
            if abs(pe_change) > 5:
                trend_text = "improving" if pe_change < 0 else "increasing"
                st.info(f"ðŸ“Š P/E ratio {trend_text}: {pe_change:+.1f}% since {prev_date}")

    with col2:
        st.markdown("**âš ï¸ Areas for Attention**")

        attention_areas = []
        for name, score in component_scores:
            if score < 50:
                attention_areas.append((name, score, "warning"))
            elif score < 60:
                attention_areas.append((name, score, "info"))

        if attention_areas:
            for name, score, alert_type in attention_areas:
                if alert_type == "warning":
                    st.warning(f"â€¢ {name} metrics need attention ({score:.1f}/100)")
                else:
                    st.info(f"â€¢ {name} has room for improvement ({score:.1f}/100)")
        else:
            weakest_scores = component_scores[-2:]
            st.info("**ðŸ“Š Areas to Monitor:**")
            for name, score in weakest_scores:
                relative_weakness = "lowest" if score == component_scores[-1][1] else "second lowest"
                st.info(f"â€¢ {name}: {score:.1f}/100 ({relative_weakness})")
            st.success(f"ðŸ’ª Overall strong performance - all metrics above 60")

def show_methodology_guide():
    """Display the methodology guide page."""
    st.title("ðŸ“š Methodology Guide")
    st.markdown("### Understanding Our 4-Component Stock Analysis System")

    # Overview section
    st.header("ðŸŽ¯ Overview")
    st.markdown("""
    Our stock analysis methodology evaluates companies using four key components, each weighted based on investment importance:

    - **ðŸ¢ Fundamental Analysis (40%)** - Traditional valuation metrics
    - **ðŸ’Ž Quality Metrics (25%)** - Financial health and efficiency
    - **ðŸ“ˆ Growth Analysis (20%)** - Revenue and earnings momentum
    - **ðŸ’­ Sentiment Analysis (15%)** - Market perception and momentum

    Each component is scored 0-100, then combined into a weighted composite score for ranking.
    """)

    # Fundamental Analysis section
    st.header("ðŸ¢ Fundamental Analysis (40% Weight)")
    st.markdown("""
    **Key Metrics:**
    - **P/E Ratio**: Current valuation relative to earnings
    - **EV/EBITDA**: Enterprise value compared to operating earnings
    - **PEG Ratio**: Growth-adjusted valuation metric
    - **FCF Yield**: Free cash flow relative to market value

    **Scoring**: Lower ratios (indicating cheaper valuations) receive higher scores.

    **Investment Insight**: High fundamental scores suggest stocks trading below intrinsic value.
    """)

    # Quality Analysis section
    st.header("ðŸ’Ž Quality Analysis (25% Weight)")
    st.markdown("""
    **Key Metrics:**
    - **ROE (Return on Equity)**: Profitability efficiency
    - **ROIC (Return on Invested Capital)**: Capital allocation effectiveness
    - **Debt-to-Equity**: Financial leverage and risk
    - **Current Ratio**: Short-term liquidity and financial stability

    **Scoring**: Higher profitability ratios and lower debt levels receive higher scores.

    **Investment Insight**: High quality scores indicate companies with strong fundamentals and lower financial risk.
    """)

    # Growth Analysis section
    st.header("ðŸ“ˆ Growth Analysis (20% Weight)")
    st.markdown("""
    **Key Metrics:**
    - **Revenue Growth**: Top-line expansion trends
    - **EPS Growth**: Earnings per share improvement
    - **Growth Stability**: Consistency of growth patterns
    - **Forward Projections**: Analyst expectations for future growth

    **Scoring**: Higher and more consistent growth rates receive higher scores.

    **Investment Insight**: High growth scores suggest companies with expanding business momentum.
    """)

    # Sentiment Analysis section
    st.header("ðŸ’­ Sentiment Analysis (15% Weight)")
    st.markdown("""
    **Key Metrics:**
    - **News Sentiment**: Media coverage analysis using NLP
    - **Social Media Mentions**: Reddit and social platform discussions
    - **Volume Patterns**: Trading activity as sentiment indicator
    - **Analyst Revisions**: Professional opinion changes

    **Scoring**: Positive sentiment trends and increasing attention receive higher scores.

    **Investment Insight**: High sentiment scores may indicate building momentum or emerging opportunities.
    """)

    # Score Interpretation section
    st.header("ðŸ“Š Score Interpretation")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ðŸŸ¢ High Scores (70-100)")
        st.markdown("""
        **Interpretation**: Potentially undervalued opportunities
        - Strong fundamentals at reasonable prices
        - High-quality business operations
        - Positive growth trajectories
        - Favorable market sentiment

        **Investment Consideration**: Consider for value-oriented portfolios
        """)

    with col2:
        st.subheader("ðŸ”´ Low Scores (0-40)")
        st.markdown("""
        **Interpretation**: Potentially overvalued or problematic
        - Expensive relative to fundamentals
        - Weak financial health indicators
        - Declining or stagnant growth
        - Negative market perception

        **Investment Consideration**: Avoid or consider for short positions
        """)

    # Advanced Features section
    st.header("ðŸ”§ Advanced Features")

    st.subheader("âš–ï¸ Custom Weight Adjustment")
    st.markdown("""
    Use the sidebar sliders to adjust component weights based on your investment philosophy:

    - **Value Investors**: Increase Fundamental weight (50-60%)
    - **Quality Investors**: Increase Quality weight (35-40%)
    - **Growth Investors**: Increase Growth weight (30-35%)
    - **Momentum Investors**: Increase Sentiment weight (25-30%)

    The system automatically normalizes weights and shows ranking changes in real-time.
    """)

    st.subheader("ðŸ“ˆ Ranking Analysis")
    st.markdown("""
    **Original Rankings**: Based on our research-backed 40/25/20/15 allocation

    **Custom Rankings**: Reflects your personalized weight preferences

    **Movement Analysis**:
    - **Green arrows**: Stocks improving in custom rankings
    - **Red arrows**: Stocks declining in custom rankings
    - **Position changes**: Show which stocks benefit from your investment style
    """)

    st.subheader("ðŸ“Š Component Deep-Dive")
    st.markdown("""
    Each component provides detailed sub-metrics:

    **Fundamental Metrics**:
    - **Forward P/E**: Future earnings-based valuation
    - **PEG**: Growth-adjusted P/E for growth stocks
    - **FCF Yield**: Cash generation relative to price

    **Quality Indicators**:
    - **ROE Trend**: Return on equity trajectory
    - **Debt Coverage**: Ability to service debt obligations
    - **Working Capital**: Short-term financial health

    **Growth Patterns**:
    - **Revenue Acceleration**: Quarter-over-quarter changes
    - **Margin Expansion**: Profitability improvements
    - **Guidance Quality**: Management forecast reliability

    **Sentiment Dynamics**:
    - **News Velocity**: Rate of coverage changes
    - **Social Momentum**: Discussion volume trends
    - **Institutional Interest**: Smart money positioning
    """)

    # Risk and Limitations section
    st.header("âš ï¸ Important Considerations")

    st.markdown("""
    **Risk Factors**:
    - Past performance doesn't guarantee future results
    - Market conditions can override fundamental analysis
    - Sentiment can be volatile and change rapidly
    - Some metrics may lag actual business performance

    **Data Limitations**:
    - **Historical Focus**: Analysis based on trailing data
    - **Market Coverage**: Primarily S&P 500 constituents
    - **Update Frequency**: Daily refreshes may miss intraday developments
    - **Sector Bias**: Some sectors may inherently score higher/lower

    **Best Practices**:
    - Use scores as starting point for deeper research
    - Consider multiple time horizons for investment decisions
    - Diversify across sectors and market capitalizations
    - Monitor position sizing and risk management
    - Combine with additional due diligence and analysis
    """)

    # Methodology Evolution section
    st.header("ðŸ”„ Methodology Evolution")

    st.markdown("""
    **Version History**:
    - **v1.0**: Basic 4-component framework
    - **v1.1**: Enhanced sentiment analysis with Reddit integration
    - **v1.2**: Improved data quality controls and fallback mechanisms
    - **v2.0**: Real-time weight customization and ranking comparison

    **Continuous Improvement**:
    - Regular backtesting against market performance
    - Incorporation of new data sources and metrics
    - Refinement of scoring algorithms based on effectiveness
    - User feedback integration for enhanced usability
    """)

    # Final Disclaimer
    st.header("ðŸ“‹ Final Notes")

    st.markdown("""
    **Investment Philosophy**: This tool supports multiple investment approaches while maintaining analytical rigor.

    **Decision Support**: Designed to enhance, not replace, fundamental investment research and professional judgment.

    **Continuous Learning**: Methodology adapts based on market feedback and performance validation.

    **User Empowerment**: Provides transparency and customization for informed decision-making.

    Remember to consider:
    - Individual investment goals and risk tolerance
    - Current market conditions and cycles
    - Portfolio diversification needs

    **Disclaimer**: This tool is for educational and research purposes only. Not investment advice.
    """)

# Data Management Functions
@st.cache_resource
def initialize_data_collection():
    """Initialize data collection orchestrator"""
    try:
        from src.data.collectors import DataCollectionOrchestrator
        return DataCollectionOrchestrator()
    except Exception as e:
        st.error(f"Could not initialize data collection: {e}")
        return None

def get_data_source_status():
    """Get status of data sources with counts and freshness"""
    try:
        conn = sqlite3.connect('data/stock_data.db')
        sources = {}

        # Helper function to calculate status
        def calculate_status(last_update_str):
            if last_update_str == 'Never' or not last_update_str:
                return {'status': 'ðŸ”´ No Data', 'days_old': 999}

            try:
                # Parse timestamp - handle both ISO and space-separated formats
                if 'T' in last_update_str:
                    last_update = datetime.strptime(last_update_str.split('.')[0], '%Y-%m-%dT%H:%M:%S')
                elif ':' in last_update_str:
                    # Handle full datetime with time
                    last_update = datetime.strptime(last_update_str, '%Y-%m-%d %H:%M:%S')
                else:
                    # Handle date-only
                    last_update = datetime.strptime(last_update_str, '%Y-%m-%d')

                # Calculate age - handle negative values (future dates due to timezone)
                time_diff = datetime.now() - last_update
                days_old = max(0, time_diff.days)  # Ensure non-negative
                hours_old = time_diff.total_seconds() / 3600

                # More granular freshness for recent updates
                if abs(hours_old) < 1:  # Within 1 hour (including future)
                    status = 'ðŸŸ¢ Fresh'
                    days_old = 0
                elif days_old == 0:
                    status = 'ðŸŸ¢ Fresh'
                elif days_old <= 7:
                    status = 'ðŸŸ¡ Recent'
                else:
                    status = 'ðŸ”´ Stale'

                return {'status': status, 'days_old': days_old}
            except:
                return {'status': 'ðŸ”´ Error', 'days_old': 999}

        # Check fundamental data
        fundamental_query = "SELECT COUNT(*) as count, MAX(created_at) as latest FROM fundamental_data"
        result = pd.read_sql_query(fundamental_query, conn)
        latest = result.iloc[0]['latest'] if result.iloc[0]['latest'] else 'Never'
        status_info = calculate_status(latest)
        sources['Fundamentals'] = {
            'count': result.iloc[0]['count'],
            'last_update': latest,
            'status': status_info['status'],
            'days_old': status_info['days_old']
        }

        # Check price data
        price_query = "SELECT COUNT(*) as count, MAX(date) as latest FROM price_data"
        result = pd.read_sql_query(price_query, conn)
        latest = result.iloc[0]['latest'] if result.iloc[0]['latest'] else 'Never'
        status_info = calculate_status(latest)
        sources['Prices'] = {
            'count': result.iloc[0]['count'],
            'last_update': latest,
            'status': status_info['status'],
            'days_old': status_info['days_old']
        }

        # Check news data
        news_query = "SELECT COUNT(*) as count, MAX(publish_date) as latest FROM news_articles"
        result = pd.read_sql_query(news_query, conn)
        latest = result.iloc[0]['latest'] if result.iloc[0]['latest'] else 'Never'
        status_info = calculate_status(latest)
        sources['News'] = {
            'count': result.iloc[0]['count'],
            'last_update': latest,
            'status': status_info['status'],
            'days_old': status_info['days_old']
        }

        # Check Reddit data
        reddit_query = "SELECT COUNT(*) as count, MAX(created_utc) as latest FROM reddit_posts"
        result = pd.read_sql_query(reddit_query, conn)
        latest = result.iloc[0]['latest'] if result.iloc[0]['latest'] else 'Never'
        status_info = calculate_status(latest)
        sources['Reddit'] = {
            'count': result.iloc[0]['count'],
            'last_update': latest,
            'status': status_info['status'],
            'days_old': status_info['days_old']
        }

        # Check calculated metrics
        metrics_query = "SELECT COUNT(*) as count, MAX(created_at) as latest FROM calculated_metrics"
        result = pd.read_sql_query(metrics_query, conn)
        latest = result.iloc[0]['latest'] if result.iloc[0]['latest'] else 'Never'
        status_info = calculate_status(latest)
        sources['Calculated Metrics'] = {
            'count': result.iloc[0]['count'],
            'last_update': latest,
            'status': status_info['status'],
            'days_old': status_info['days_old']
        }

        conn.close()
        return sources

    except Exception as e:
        st.error(f"Error checking data source status: {e}")
        return {}

def run_data_refresh(data_types: List[str], symbols: Optional[List[str]] = None, progress_placeholder=None):
    """Run data refresh with progress tracking"""
    try:
        orchestrator = initialize_data_collection()
        if not orchestrator:
            return False, "Could not initialize data collection system"

        # Get symbols to refresh (default to all if none specified)
        if not symbols:
            conn = sqlite3.connect('data/stock_data.db')
            symbol_query = "SELECT DISTINCT symbol FROM stocks ORDER BY symbol"
            symbols_df = pd.read_sql_query(symbol_query, conn)
            symbols = symbols_df['symbol'].tolist()  # Use all stocks for production
            conn.close()

        results = {}
        total_steps = len(data_types)

        for i, data_type in enumerate(data_types):
            if progress_placeholder:
                progress = (i + 1) / total_steps
                progress_placeholder.progress(progress, f"Refreshing {data_type}...")

            try:
                if data_type == 'fundamentals':
                    result = orchestrator.refresh_fundamentals_only(symbols)
                elif data_type == 'prices':
                    result = orchestrator.refresh_prices_only(symbols)
                elif data_type == 'news':
                    result = orchestrator.refresh_news_only(symbols)
                elif data_type == 'sentiment':
                    result = orchestrator.refresh_sentiment_only(symbols)
                else:
                    result = {symbol: False for symbol in symbols}

                results[data_type] = result

            except Exception as e:
                results[data_type] = f"Error: {str(e)}"

        if progress_placeholder:
            progress_placeholder.progress(1.0, "Refresh completed!")

        return True, results

    except Exception as e:
        return False, f"Refresh failed: {str(e)}"

def show_data_management():
    """Display the reorganized 3-step data workflow interface"""
    st.header("ðŸ—„ï¸ Data Management - 3-Step Workflow")

    st.info("""
    **ðŸŽ¯ Streamlined Process:** Collect Data â†’ Process Sentiment â†’ Calculate Rankings

    Each step serves a specific purpose and should be completed in order for best results.
    """)

    # === S&P 500 UNIVERSE MANAGEMENT ===
    st.markdown("---")
    st.markdown("## ðŸ“Š S&P 500 Universe Management")
    st.markdown("*Track S&P 500 composition changes and manage stock universe*")

    # Initialize database manager
    from src.data.database import DatabaseManager
    db = DatabaseManager()

    # Import sync utility
    try:
        from utilities.sync_sp500 import SP500Syncer
        sp500_syncer = SP500Syncer(db_path='data/stock_data.db')
    except Exception as e:
        st.error(f"âŒ Could not initialize S&P 500 syncer: {e}")
        sp500_syncer = None

    # Get current status from database
    universe_status = {"total": 0, "active": 0, "inactive": 0}
    last_check_date = None

    if db.connect():
        cursor = db.connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM stocks")
        universe_status["total"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM stocks WHERE is_active = 1")
        universe_status["active"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM stocks WHERE is_active = 0")
        universe_status["inactive"] = cursor.fetchone()[0]

        cursor.close()
        db.close()

    # Show current status
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ðŸ“Š Total Stocks", f"{universe_status['total']:,}")
    with col2:
        st.metric("âœ… Active (S&P 500)", f"{universe_status['active']:,}")
    with col3:
        st.metric("â¸ï¸ Inactive (Removed)", f"{universe_status['inactive']:,}")
    with col4:
        expected_count = 503
        delta = universe_status['active'] - expected_count
        delta_str = f"+{delta}" if delta > 0 else str(delta) if delta < 0 else "On Target"
        st.metric("ðŸŽ¯ Expected", f"{expected_count}", delta=delta_str)

    # S&P 500 sync interface
    st.markdown("**Manage S&P 500 Composition:**")

    col1, col2 = st.columns([3, 1])

    with col1:
        st.markdown("â€¢ ðŸ” **Check:** Fetch current S&P 500 list and compare with database")
        st.markdown("â€¢ âœ… **Auto-marks:** Removed stocks as inactive (preserves historical data)")
        st.markdown("â€¢ âž• **Auto-adds:** New S&P 500 additions with full company info")
        st.markdown("â€¢ ðŸ”’ **Safe:** Preview changes before applying")

    with col2:
        st.markdown("**Actions:**")

        # Initialize session state for S&P 500 changes
        if 'sp500_changes' not in st.session_state:
            st.session_state.sp500_changes = None
        if 'sp500_metadata' not in st.session_state:
            st.session_state.sp500_metadata = None

        check_button = st.button("ðŸ” Check S&P 500", type="secondary", help="Check for composition changes", use_container_width=True)

        if check_button and sp500_syncer:
            with st.spinner("Fetching current S&P 500 composition..."):
                try:
                    # Fetch current S&P 500
                    current_symbols, metadata = sp500_syncer.fetch_sp500_symbols()

                    if current_symbols:
                        # Get database stocks
                        all_db_symbols, active_db_symbols = sp500_syncer.get_database_stocks()

                        # Detect changes
                        changes = sp500_syncer.detect_changes(set(current_symbols), active_db_symbols)

                        # Store in session state
                        st.session_state.sp500_changes = changes
                        st.session_state.sp500_metadata = metadata

                        st.success(f"âœ… Checked S&P 500 from {metadata['source']}")
                        st.rerun()
                    else:
                        st.error("âŒ Failed to fetch S&P 500 symbols")
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")

    # Display detected changes
    if st.session_state.sp500_changes:
        changes = st.session_state.sp500_changes
        metadata = st.session_state.sp500_metadata

        st.markdown("---")
        st.markdown("### ðŸ“‹ Detected Changes")

        if changes['added'] or changes['removed']:
            col1, col2 = st.columns(2)

            with col1:
                if changes['removed']:
                    st.markdown(f"**âž– To Mark Inactive:** {len(changes['removed'])} stocks")
                    with st.expander(f"View {len(changes['removed'])} stocks to be marked inactive"):
                        for symbol in sorted(changes['removed']):
                            st.markdown(f"â€¢ `{symbol}`")
                else:
                    st.success("**No stocks to remove**")

            with col2:
                if changes['added']:
                    st.markdown(f"**âž• To Add:** {len(changes['added'])} new stocks")
                    with st.expander(f"View {len(changes['added'])} new stocks"):
                        for symbol in sorted(changes['added']):
                            st.markdown(f"â€¢ `{symbol}`")
                else:
                    st.success("**No stocks to add**")

            # Apply changes button
            st.markdown("---")
            col1, col2 = st.columns([3, 1])

            with col1:
                st.info(f"ðŸ“Š **Source:** {metadata.get('source', 'Unknown')} ({metadata.get('total_symbols', 0)} symbols)")
                st.info(f"âš ï¸ **Action:** Will mark {len(changes['removed'])} as inactive, add {len(changes['added'])} new stocks")

            with col2:
                if st.button("âœ… Apply Changes", type="primary", help="Apply S&P 500 changes to database", use_container_width=True):
                    with st.spinner("Applying S&P 500 changes..."):
                        try:
                            results = sp500_syncer.apply_changes(changes, dry_run=False)
                            st.success(f"âœ… Changes applied successfully!")
                            st.success(f"ðŸ“Š Marked {results['removed']} as inactive, added {results['added']} new stocks")

                            # Clear session state
                            st.session_state.sp500_changes = None
                            st.session_state.sp500_metadata = None

                            st.info("ðŸ”„ Refreshing page...")
                            time.sleep(2)
                            st.rerun()
                        except Exception as e:
                            st.error(f"âŒ Error applying changes: {str(e)}")
        else:
            st.success("âœ… **No changes detected** - S&P 500 composition is up to date!")
            st.info(f"ðŸ“Š Current: {changes['current_total']} stocks | Database: {changes['database_total']} active")

    # Get current data status
    data_status = {
        "stocks_count": 0, "fundamentals_count": 0, "news_count": 0, "reddit_count": 0,
        "news_no_sentiment": 0, "reddit_no_sentiment": 0, "news_with_sentiment": 0, "reddit_with_sentiment": 0
    }

    if db.connect():
        cursor = db.connection.cursor()

        # Get basic counts
        cursor.execute("SELECT COUNT(*) FROM stocks")
        data_status["stocks_count"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM fundamental_data")
        data_status["fundamentals_count"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM news_articles")
        data_status["news_count"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM reddit_posts")
        data_status["reddit_count"] = cursor.fetchone()[0]

        # Get sentiment status - Only count NULL as unprocessed (0.0 is a valid neutral sentiment)
        cursor.execute("SELECT COUNT(*) FROM news_articles WHERE sentiment_score IS NULL")
        data_status["news_no_sentiment"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM reddit_posts WHERE sentiment_score IS NULL")
        data_status["reddit_no_sentiment"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM news_articles WHERE sentiment_score IS NOT NULL")
        data_status["news_with_sentiment"] = cursor.fetchone()[0]

        cursor.execute("SELECT COUNT(*) FROM reddit_posts WHERE sentiment_score IS NOT NULL")
        data_status["reddit_with_sentiment"] = cursor.fetchone()[0]

        cursor.close()
        db.close()

    # === STEP 1: COLLECT RAW DATA ===
    st.markdown("---")
    st.markdown("## ðŸ“¥ Step 1: Collect Raw Data")
    st.markdown("*Gather latest market data WITHOUT calculating sentiment*")

    # Show current data status
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ðŸ“Š Stocks Tracked", f"{data_status['stocks_count']:,}")
    with col2:
        st.metric("ðŸ’° Fundamentals", f"{data_status['fundamentals_count']:,}")
    with col3:
        st.metric("ðŸ“° News Articles", f"{data_status['news_count']:,}")
    with col4:
        st.metric("ðŸ’­ Reddit Posts", f"{data_status['reddit_count']:,}")

    # Data collection options
    col1, col2 = st.columns([3, 1])

    with col1:
        st.markdown("**What gets collected:**")
        st.markdown("â€¢ ðŸ“Š **Fundamentals:** P/E ratios, market cap, revenue, financial metrics")
        st.markdown("â€¢ ðŸ’¹ **Prices:** Historical price data and trading volumes")
        st.markdown("â€¢ ðŸ“° **News:** Headlines and summaries (sentiment_score = NULL)")
        st.markdown("â€¢ ðŸ’­ **Reddit:** Posts and discussions (sentiment_score = NULL)")
        st.markdown("")
        st.markdown("âš ï¸ **Important:** Sentiment analysis happens in Step 2 via Claude API")

    with col2:
        st.markdown("**Quick Actions:**")

        if st.button("ðŸš€ Collect All Data", type="primary", help="Collect fundamentals, prices, news, and Reddit data"):
            with st.spinner("Collecting all data types..."):
                try:
                    data_types = ['fundamentals', 'prices', 'news', 'sentiment']
                    success, results = run_data_refresh(data_types, None, None)
                    if success:
                        st.success("âœ… Data collection completed!")
                        st.info("âž¡ï¸ **Next:** Proceed to Step 2 for sentiment processing")
                        st.rerun()
                    else:
                        st.error(f"âŒ Collection failed: {results}")
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")

        if st.button("âš¡ Quick Update", help="Update fundamentals and prices only"):
            with st.spinner("Quick update in progress..."):
                try:
                    data_types = ['fundamentals', 'prices']
                    success, results = run_data_refresh(data_types, None, None)
                    if success:
                        st.success("âœ… Quick update completed!")
                        st.rerun()
                    else:
                        st.error(f"âŒ Update failed: {results}")
                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")

    # Advanced options
    with st.expander("ðŸ”§ Advanced Collection Options"):
        st.markdown("**Select specific data types:**")

        col1, col2 = st.columns(2)
        with col1:
            collect_fundamentals = st.checkbox("ðŸ“Š Fundamentals", value=True)
            collect_prices = st.checkbox("ðŸ’¹ Prices", value=True)
        with col2:
            collect_news = st.checkbox("ðŸ“° News Articles", value=False)
            collect_reddit = st.checkbox("ðŸ’­ Reddit Posts", value=False)

        selected_symbols = st.multiselect(
            "ðŸŽ¯ Specific Symbols (optional):",
            options=['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA', 'META', 'TSLA'],
            help="Leave empty to update all stocks"
        )

        if st.button("ðŸ”„ Custom Collection"):
            data_types = []
            if collect_fundamentals: data_types.append('fundamentals')
            if collect_prices: data_types.append('prices')
            if collect_news: data_types.append('news')
            if collect_reddit: data_types.append('sentiment')

            if data_types:
                with st.spinner("Running custom collection..."):
                    try:
                        symbols = selected_symbols if selected_symbols else None
                        success, results = run_data_refresh(data_types, symbols, None)
                        if success:
                            st.success("âœ… Custom collection completed!")
                            st.rerun()
                        else:
                            st.error(f"âŒ Collection failed: {results}")
                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")
            else:
                st.warning("âš ï¸ Please select at least one data type")

    # === STEP 2: PROCESS SENTIMENT ===
    st.markdown("---")
    st.markdown("## ðŸ¤– Step 2: Process Sentiment via Claude API")
    st.markdown("*High-quality financial sentiment analysis using bulk processing*")

    # Show sentiment status
    total_unprocessed = data_status['news_no_sentiment'] + data_status['reddit_no_sentiment']
    total_processed = data_status['news_with_sentiment'] + data_status['reddit_with_sentiment']

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("ðŸ“° News Need Sentiment", f"{data_status['news_no_sentiment']:,}")
    with col2:
        st.metric("ðŸ’­ Reddit Need Sentiment", f"{data_status['reddit_no_sentiment']:,}")
    with col3:
        st.metric("ðŸ”„ Total Unprocessed", f"{total_unprocessed:,}")
    with col4:
        st.metric("âœ… Already Processed", f"{total_processed:,}")

    # Initialize bulk processor
    @st.cache_resource
    def get_bulk_processor():
        """Initialize the unified bulk processor"""
        try:
            from src.data.unified_bulk_processor import UnifiedBulkProcessor
            import os
            api_key = os.getenv('NEWS_API_KEY') or os.getenv('ANTHROPIC_API_KEY')
            if not api_key:
                return None
            return UnifiedBulkProcessor(api_key)
        except Exception as e:
            st.error(f"Could not initialize bulk processor: {e}")
            return None

    bulk_processor = get_bulk_processor()

    if not bulk_processor:
        st.error("âŒ Bulk sentiment processor not available")
        st.info("ðŸ”‘ **API Key Required**: Set NEWS_API_KEY or ANTHROPIC_API_KEY in your .env file")
        st.info("ðŸ’¡ **Why needed**: Claude API provides superior financial sentiment analysis")

    else:
        # === STEP 2 COMPLETION STATUS ===
        step2_complete = (total_unprocessed == 0)

        if step2_complete:
            st.success("### âœ… STEP 2: COMPLETE")
            st.success("ðŸŽ‰ **All sentiment processing finished!** All {total_processed:,} items have been analyzed.")
            st.info("âž¡ï¸ **Ready for Step 3:** Proceed to calculate final rankings below")

            # Show summary stats
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ðŸ“° News Processed", f"{data_status['news_with_sentiment']:,}")
            with col2:
                st.metric("ðŸ’­ Reddit Processed", f"{data_status['reddit_with_sentiment']:,}")
            with col3:
                st.metric("âœ… Total Complete", f"{total_processed:,}")
        else:
            st.warning("### â³ STEP 2: IN PROGRESS")
            st.info(f"ðŸ“Š **Status:** {total_processed:,} processed, {total_unprocessed:,} remaining")

        st.markdown("---")

        # Check for active batches
        active_batches = []
        if db.connect():
            active_batches = db.get_active_batch_ids()
            db.close()

        if active_batches:
            st.markdown("### ðŸ“Š Active Batch Monitoring")

            selected_batch = st.selectbox(
                "Select batch to monitor:",
                active_batches,
                format_func=lambda x: f"Batch: {x[:20]}..."
            )

            if selected_batch:
                col1, col2 = st.columns([2, 1])

                with col1:
                    # Check batch status
                    batch_status = bulk_processor.check_batch_status(selected_batch)
                    if batch_status and batch_status['success']:
                        st.info(f"**Status:** {batch_status['status']}")
                        st.info(f"**Progress:** {batch_status['completed_count']}/{batch_status['submitted_count']} items processed")

                        if batch_status['status'] == 'ended':
                            st.success("ðŸŽ‰ Batch completed successfully!")
                    else:
                        st.warning("âš ï¸ Could not check batch status")

                with col2:
                    if batch_status and batch_status.get('status') == 'ended':
                        if st.button("ðŸ“¥ Process Results", type="primary"):
                            with st.spinner("Retrieving and processing batch results..."):
                                try:
                                    result = bulk_processor.retrieve_and_process_batch_results(selected_batch)
                                    if result and result.get('success'):
                                        st.success(f"âœ… Results processed successfully!")
                                        st.success(f"ðŸ“Š Updated {result.get('successful_updates', 0)} items")
                                        if result.get('failed_updates', 0) > 0:
                                            st.warning(f"âš ï¸ {result.get('failed_updates')} items failed to process")
                                        st.info("âž¡ï¸ **Next:** Proceed to Step 3 for final calculations")
                                        st.rerun()
                                    else:
                                        st.error(f"âŒ Failed to process results: {result.get('error', 'Unknown error') if result else 'No result returned'}")
                                except Exception as e:
                                    st.error(f"âŒ Error: {str(e)}")

        elif total_unprocessed > 0:
            st.markdown("### ðŸš€ Submit New Batch")

            # Check for recently submitted batches (within last 5 minutes)
            recent_batch_warning = False
            if db.connect():
                cursor = db.connection.cursor()
                cursor.execute("""
                    SELECT batch_id, COUNT(*) as count, MAX(created_at) as recent
                    FROM batch_mapping
                    WHERE created_at > datetime('now', '-5 minutes')
                    GROUP BY batch_id
                """)
                recent_batches = cursor.fetchall()
                cursor.close()
                db.close()

                if recent_batches:
                    recent_batch_warning = True
                    st.warning(f"âš ï¸ **Recent batch detected!** A batch was submitted {recent_batches[0][2]} with {recent_batches[0][1]} items.")
                    st.info("ðŸ’¡ Wait a few seconds and refresh the page to see the batch monitoring section above.")
                    st.info("ðŸ”„ If you don't see it after refreshing, the batch may not have been stored properly.")

            col1, col2 = st.columns([2, 1])

            with col1:
                st.markdown("**Claude API Batch Processing:**")
                st.markdown(f"â€¢ ðŸ“Š **Total Items:** {total_unprocessed:,} items ready for processing")
                st.markdown("â€¢ ðŸ’° **Cost:** 50% savings vs individual API calls")
                st.markdown("â€¢ â±ï¸ **Time:** Typically completes in <1 hour")
                st.markdown("â€¢ ðŸ§  **Quality:** Superior financial context understanding")

            with col2:
                button_disabled = recent_batch_warning
                button_help = "A batch was just submitted. Refresh page to monitor it." if recent_batch_warning else f"Submit {total_unprocessed:,} items for sentiment analysis"

                if st.button("ðŸš€ Submit Batch", type="primary", help=button_help, disabled=button_disabled):
                    with st.spinner("Preparing and submitting batch..."):
                        try:
                            # Get unprocessed items
                            if db.connect():
                                unprocessed_items = db.get_unprocessed_items_for_batch()
                                db.close()

                                if unprocessed_items:
                                    # Prepare data for submission
                                    news_articles = []
                                    reddit_posts = []

                                    for item in unprocessed_items:
                                        if item['content_type'] == 'news':
                                            news_articles.append((
                                                item['symbol'],
                                                item['title'],
                                                item['content'],
                                                {'record_id': item['record_id']}
                                            ))
                                        elif item['content_type'] == 'reddit':
                                            reddit_posts.append((
                                                item['symbol'],
                                                item['title'],
                                                item['content'],
                                                {'record_id': item['record_id']}
                                            ))

                                    # Submit batch
                                    submission = bulk_processor.bulk_processor.submit_batch_for_processing(
                                        news_articles=news_articles if news_articles else None,
                                        reddit_posts=reddit_posts if reddit_posts else None
                                    )

                                    if submission:
                                        batch_id, requests = submission
                                        # Store batch mapping
                                        bulk_processor._store_batch_mapping(batch_id, requests)
                                        st.success(f"âœ… Batch submitted successfully!")
                                        st.info(f"ðŸ“Š Processing {len(requests)} items")
                                        st.info(f"ðŸ†” Batch ID: {batch_id[:20]}...")
                                        st.info("â±ï¸ Expected completion: <1 hour")
                                        st.info("ðŸ”„ Refresh page to monitor progress")
                                        st.rerun()
                                    else:
                                        st.error("âŒ Failed to submit batch")
                                else:
                                    st.error("âŒ No unprocessed items found")
                            else:
                                st.error("âŒ Could not connect to database")
                        except Exception as e:
                            st.error(f"âŒ Error submitting batch: {str(e)}")

        else:
            st.success("âœ… All items have sentiment scores! No processing needed.")

    # === MANUAL BATCH RECOVERY ===
    if not active_batches and total_unprocessed > 0:
        with st.expander("ðŸ”§ Manual Batch Recovery (Advanced)", expanded=False):
            st.markdown("""
            **If you submitted a batch but it's not showing above:**

            This can happen if the batch was submitted through another method or if there was a database issue.
            You can manually enter your batch ID from the Anthropic console to process results.
            """)

            manual_batch_id = st.text_input(
                "Enter Batch ID from Anthropic Console:",
                placeholder="msgbatch_01ABC...",
                help="Find this in your Anthropic console under Message Batches"
            )

            if manual_batch_id and st.button("ðŸ” Check & Process Manual Batch", type="secondary"):
                with st.spinner("Checking batch status..."):
                    try:
                        status = bulk_processor.check_batch_status(manual_batch_id)
                        if status and status['success']:
                            st.info(f"**Status:** {status['status']}")
                            st.info(f"**Progress:** {status['completed_count']}/{status['submitted_count']} items")

                            if status['status'] == 'ended':
                                st.success("ðŸŽ‰ Batch is complete! Processing results...")
                                result = bulk_processor.retrieve_and_process_batch_results(manual_batch_id)
                                if result and result.get('success'):
                                    st.success(f"âœ… Results processed successfully!")
                                    st.success(f"ðŸ“Š Updated {result.get('successful_updates', 0)} items")
                                    if result.get('failed_updates', 0) > 0:
                                        st.warning(f"âš ï¸ {result.get('failed_updates')} items failed to process")
                                    st.info("âž¡ï¸ **Next:** Proceed to Step 3 for final calculations")
                                    st.rerun()
                                else:
                                    st.error(f"âŒ Failed to process results: {result.get('error', 'Unknown error')}")
                            else:
                                st.warning(f"â³ Batch is still {status['status']}. Wait until it completes before processing.")
                        else:
                            st.error(f"âŒ Could not find batch: {manual_batch_id}")
                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")

    # === BATCH MONITORING SECTION ===
    if active_batches or total_unprocessed == 0:
        st.markdown("---")
        st.markdown("## ðŸ“Š Batch Processing Status")
        st.markdown("*Monitor your Claude API batch progress in real-time*")

        if active_batches:
            st.info("ðŸ”„ **Active Batch Processing** - Monitor progress below")

            # Batch status overview
            col1, col2, col3 = st.columns(3)

            # Get detailed status for all active batches
            batch_statuses = {}
            for batch_id in active_batches:
                status = bulk_processor.check_batch_status(batch_id)
                if status and status['success']:
                    batch_statuses[batch_id] = status

            # Calculate overall progress
            total_submitted = sum(s.get('submitted_count', 0) for s in batch_statuses.values())
            total_completed = sum(s.get('completed_count', 0) for s in batch_statuses.values())
            overall_progress = (total_completed / total_submitted * 100) if total_submitted > 0 else 0

            with col1:
                st.metric("ðŸ“¦ Active Batches", len(active_batches))
            with col2:
                st.metric("ðŸ“Š Total Items", f"{total_submitted:,}")
            with col3:
                st.metric("âœ… Progress", f"{overall_progress:.1f}%")

            # Individual batch details
            st.markdown("### ðŸ“‹ Batch Details")

            for batch_id, status_info in batch_statuses.items():
                with st.expander(f"ðŸ” Batch {batch_id[:20]}... - {status_info.get('status', 'unknown').title()}", expanded=True):
                    col1, col2, col3, col4 = st.columns(4)

                    with col1:
                        st.metric("Status", status_info.get('status', 'unknown').title())
                    with col2:
                        submitted = status_info.get('submitted_count', 0)
                        st.metric("Submitted", f"{submitted:,}")
                    with col3:
                        completed = status_info.get('completed_count', 0)
                        st.metric("Completed", f"{completed:,}")
                    with col4:
                        progress = (completed / submitted * 100) if submitted > 0 else 0
                        st.metric("Progress", f"{progress:.1f}%")

                    # Progress bar
                    st.progress(progress / 100.0)

                    # Status-specific actions
                    if status_info.get('status') == 'ended':
                        st.success("ðŸŽ‰ **Batch Complete!** Ready to process results.")

                        col1, col2 = st.columns([2, 1])
                        with col1:
                            st.info("ðŸ“¥ Click 'Process Results' to apply sentiment scores to your database")
                        with col2:
                            if st.button("ðŸ“¥ Process Results", key=f"process_{batch_id[:8]}", type="primary"):
                                with st.spinner("Processing batch results..."):
                                    try:
                                        result = bulk_processor.retrieve_and_process_batch_results(batch_id)
                                        if result and result.get('success'):
                                            st.success(f"âœ… Results processed successfully!")
                                            st.success(f"ðŸ“Š Updated {result.get('successful_updates', 0)} items")
                                            if result.get('failed_updates', 0) > 0:
                                                st.warning(f"âš ï¸ {result.get('failed_updates')} items failed to process")
                                            st.info("âž¡ï¸ **Next:** Proceed to Step 3 for final calculations")
                                            st.rerun()
                                        else:
                                            st.error(f"âŒ Failed to process results: {result.get('error', 'Unknown error') if result else 'No result returned'}")
                                    except Exception as e:
                                        st.error(f"âŒ Error: {str(e)}")

                    elif status_info.get('status') == 'in_progress':
                        st.info("â³ Processing in progress... Claude is analyzing your data")
                        st.markdown("**Estimated completion:** <1 hour from submission")

                        # Auto-refresh option
                        if st.button("ðŸ”„ Refresh Status", key=f"refresh_{batch_id[:8]}"):
                            st.rerun()

                    elif status_info.get('status') in ['failed', 'expired', 'cancelled']:
                        st.error(f"âŒ **Batch {status_info.get('status')}**")
                        st.info("ðŸ’¡ You may need to resubmit your data for processing")

                    else:
                        st.warning(f"âš ï¸ **Unknown status:** {status_info.get('status')}")

            # Auto-refresh controls
            st.markdown("### ðŸ”„ Auto-Refresh")
            col1, col2 = st.columns([2, 1])

            with col1:
                st.info("ðŸ’¡ **Tip:** This page will auto-update when you refresh. Batches typically complete within 1 hour.")

            with col2:
                if st.button("ðŸ”„ Refresh All Status", type="secondary"):
                    st.rerun()

        elif total_unprocessed == 0:
            st.success("âœ… **All sentiment processing complete!** No active batches.")

            # Show summary of last processing
            if db.connect():
                cursor = db.connection.cursor()

                # Get recent batch info
                cursor.execute("""
                    SELECT COUNT(DISTINCT batch_id) as batch_count,
                           COUNT(*) as total_items,
                           MAX(created_at) as last_processed
                    FROM batch_mapping
                    WHERE created_at >= datetime('now', '-7 days')
                """)

                recent_stats = cursor.fetchone()
                cursor.close()
                db.close()

                if recent_stats and recent_stats[0] > 0:
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("Recent Batches", f"{recent_stats[0]}")
                    with col2:
                        st.metric("Items Processed", f"{recent_stats[1]:,}")
                    with col3:
                        st.metric("Last Processing", recent_stats[2][:10] if recent_stats[2] else "Unknown")

    # === STEP 3: CALCULATE FINAL RANKINGS ===
    st.markdown("---")
    st.markdown("## ðŸ“Š Step 3: Calculate Final Rankings")
    st.markdown("*Generate composite scores using all collected data*")

    # Check for completed batches needing processing
    completed_batches_ready = False
    if active_batches:
        for batch_id in active_batches:
            status = bulk_processor.check_batch_status(batch_id)
            if status and status.get('status') == 'ended':
                completed_batches_ready = True
                break

    # Dynamic status messaging
    if completed_batches_ready:
        st.success("ðŸŽ‰ **Batch Processing Complete!** Process your results above before proceeding to Step 3.")
        st.info("â¬†ï¸ **Action Required:** Scroll up to the 'Batch Processing Status' section and click 'Process Results'")
    elif total_unprocessed > 0 and not active_batches:
        st.warning("âš ï¸ **Sentiment processing incomplete.** Complete Step 2 before running final calculations.")
        st.info("ðŸ’¡ Some stocks may receive preliminary scores based on available data.")
    elif active_batches:
        # Check if any are still processing
        in_progress_count = 0
        for batch_id in active_batches:
            status = bulk_processor.check_batch_status(batch_id)
            if status and status.get('status') == 'in_progress':
                in_progress_count += 1

        if in_progress_count > 0:
            st.info(f"â³ **Sentiment processing in progress** ({in_progress_count} batch{'es' if in_progress_count > 1 else ''} active). Monitor progress above.")
        else:
            st.info("â³ **Checking batch status...** Monitor progress in the section above.")
    else:
        st.success("âœ… **Ready for final calculations.** All sentiment data is up-to-date.")

    # Information section
    st.markdown("**ðŸ“‹ What gets calculated:**")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("â€¢ ðŸ“Š **Fundamental** (40%): P/E, P/B, PEG")
        st.markdown("â€¢ ðŸ† **Quality** (25%): ROE, margins")
    with col2:
        st.markdown("â€¢ ðŸ“ˆ **Growth** (20%): Revenue, earnings")
        st.markdown("â€¢ ðŸ’­ **Sentiment** (15%): News + Reddit")
    with col3:
        st.markdown("â€¢ ðŸŽ¯ **Composite Rankings**")
        st.markdown("â€¢ ðŸ“Š **Final weighted scores**")

    st.markdown("---")
    st.markdown("### ðŸŽ¬ Choose Calculation Method:")

    # Organize buttons in a 2x2 grid for better display
    col1, col2 = st.columns(2)

    with col1:
        # Test calculation with a few stocks first
        if st.button("ðŸ§ª Test Calculation", help="Test with a few stocks first (AAPL, MSFT, GOOGL)", use_container_width=True):
            with st.spinner("Testing calculation with sample stocks..."):
                try:
                    from utilities.update_analytics import update_analytics
                    import logging

                    logger = logging.getLogger('test_calc')
                    logger.setLevel(logging.INFO)

                    # Test with just 3 stocks
                    success = update_analytics(logger=logger, symbols=['AAPL', 'MSFT', 'GOOGL'], force_recalculate=True)

                    if success:
                        st.success("âœ… Test calculation successful!")
                        st.info("ðŸ’¡ Now try the full calculation below")
                    else:
                        st.error("âŒ Test calculation failed - there may be data issues")

                except Exception as e:
                    st.error(f"âŒ Test failed: {str(e)}")

        with col2:
            # Calculate All button
            if st.button("ðŸ“Š Calculate Rankings", type="primary", help="Calculate rankings for stocks needing updates", use_container_width=True):
                with st.spinner("Calculating composite scores..."):
                    try:
                        # Import and call the analytics update utility
                        from utilities.update_analytics import update_analytics
                        import logging
                        import io

                        # Create a logger for the utility with stream capture
                        logger = logging.getLogger('dashboard_metrics')
                        logger.setLevel(logging.INFO)

                        # Capture logging output
                        log_capture = io.StringIO()
                        handler = logging.StreamHandler(log_capture)
                        handler.setLevel(logging.INFO)
                        logger.addHandler(handler)

                        # Run the calculation
                        success = update_analytics(logger=logger)

                        # Get captured logs
                        log_output = log_capture.getvalue()

                        # Count successful vs failed regardless of return value
                        success_count = log_output.count("âœ…") + log_output.count("Analytics updated successfully")
                        failed_count = log_output.count("âŒ") + log_output.count("calculation failed")

                        if success:
                            st.success("âœ… Rankings calculated successfully!")
                            st.info("ðŸŽ‰ **Complete!** Check the main dashboard for updated rankings")
                        elif success_count > 0:
                            # Function returned False but still processed stocks successfully
                            st.warning("âš ï¸ Calculation completed with some limitations")
                            st.success(f"âœ… Successfully processed {success_count} stocks")
                            if failed_count > 0:
                                st.info(f"â„¹ï¸ Skipped {failed_count} stocks due to data quality issues")
                            st.info("ðŸ“Š **Result:** Rankings updated for stocks with sufficient data quality")
                        else:
                            st.error("âŒ Calculation failed - check logs for details")

                        if log_output and (not success or failed_count > 0):
                            with st.expander("ðŸ“ Show Processing Details"):
                                st.text(log_output[-2000:])  # Show last 2000 chars

                    except Exception as e:
                        st.error(f"âŒ Error during calculation: {str(e)}")
                        import traceback
                        with st.expander("ðŸ“ Show Full Error"):
                            st.text(traceback.format_exc())

    # Add spacing between rows
    st.markdown("")

    # Second row of buttons
    col_a, col_b = st.columns(2)

    with col_a:
        if st.button("ðŸŽ¯ Quality Stocks Only", help="Process only stocks with good data quality", type="secondary", use_container_width=True):
            with st.spinner("Processing stocks with good data quality..."):
                try:
                    from utilities.update_analytics import update_analytics
                    import logging

                    logger = logging.getLogger('quality_calc')
                    logger.setLevel(logging.INFO)

                    # Get stocks that already have calculated metrics (known good quality)
                    db_temp = DatabaseManager()
                    db_temp.connect()
                    cursor = db_temp.connection.cursor()
                    cursor.execute('SELECT DISTINCT symbol FROM calculated_metrics ORDER BY symbol')
                    good_stocks = [row[0] for row in cursor.fetchall()]
                    cursor.close()
                    db_temp.close()

                    if good_stocks:
                        success = update_analytics(logger=logger, symbols=good_stocks[:50], force_recalculate=True)  # Limit to 50 for speed
                        if success:
                            st.success(f"âœ… Updated {min(50, len(good_stocks))} high-quality stocks!")
                        else:
                            st.warning("âš ï¸ Some issues encountered but processing completed")
                    else:
                        st.info("â„¹ï¸ No stocks with existing calculations found")

                except Exception as e:
                    st.error(f"âŒ Error: {str(e)}")

    with col_b:
        if st.button("ðŸ”„ Force Recalculate All", type="secondary", help="Force recalculate ALL stocks regardless of data age", use_container_width=True):
            with st.spinner("Force recalculating all composite scores..."):
                try:
                    # Import and call the analytics update utility with force flag
                    from utilities.update_analytics import update_analytics
                    import logging
                    import io

                    # Create a logger for the utility with stream capture
                    logger = logging.getLogger('dashboard_metrics')
                    logger.setLevel(logging.INFO)

                    # Capture logging output
                    log_capture = io.StringIO()
                    handler = logging.StreamHandler(log_capture)
                    handler.setLevel(logging.INFO)
                    logger.addHandler(handler)

                    # Run the calculation with force_recalculate=True
                    success = update_analytics(logger=logger, force_recalculate=True)

                    # Get captured logs
                    log_output = log_capture.getvalue()

                    # Count successful vs failed regardless of return value
                    success_count = log_output.count("âœ…") + log_output.count("Analytics updated successfully")
                    failed_count = log_output.count("âŒ") + log_output.count("calculation failed")

                    if success:
                        st.success("âœ… Rankings calculated successfully!")
                        st.info("ðŸŽ‰ **Complete!** Check the main dashboard for updated rankings")
                    elif success_count > 0:
                        # Function returned False but still processed stocks successfully
                        st.warning("âš ï¸ Calculation completed with some limitations")
                        st.success(f"âœ… Successfully processed {success_count} stocks")
                        if failed_count > 0:
                            st.info(f"â„¹ï¸ Skipped {failed_count} stocks due to data quality issues")
                        st.info("ðŸ“Š **Result:** Rankings updated for stocks with sufficient data quality")
                    else:
                        st.error("âŒ Calculation failed - check logs for details")

                    if log_output and (not success or failed_count > 0):
                        with st.expander("ðŸ“ Show Processing Details"):
                            st.text(log_output[-2000:])  # Show last 2000 chars

                except Exception as e:
                    st.error(f"âŒ Error during calculation: {str(e)}")
                    import traceback
                    with st.expander("ðŸ“ Show Full Error"):
                        st.text(traceback.format_exc())

    # Show completion status
    if db.connect():
        cursor = db.connection.cursor()
        cursor.execute("SELECT COUNT(*) FROM calculated_metrics")
        metrics_count = cursor.fetchone()[0]
        cursor.close()
        db.close()

        if metrics_count > 0:
            st.info(f"ðŸ“Š **Last Update:** {metrics_count:,} stocks have calculated metrics")
        else:
            st.warning("âš ï¸ **No calculated metrics found** - run calculation to generate rankings")

    # Database Management
    st.markdown("---")
    st.markdown("## ðŸ’¾ Database Management")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("**Backup Operations:**")
        if st.button("ðŸ“ Create Backup", help="Create timestamped database backup"):
            with st.spinner("Creating backup..."):
                try:
                    import subprocess
                    result = subprocess.run(['python', 'utilities/backup_database.py'],
                                         capture_output=True, text=True, cwd='.')
                    if result.returncode == 0:
                        st.success("âœ… Database backup created successfully!")
                    else:
                        st.error(f"âŒ Backup failed: {result.stderr}")
                except Exception as e:
                    st.error(f"âŒ Backup error: {str(e)}")

    with col2:
        st.markdown("**System Status:**")
        # Show database stats
        try:
            db_stats = get_database_stats()
            if db_stats:
                for key, value in db_stats.items():
                    if key == 'coverage_pct':
                        st.metric(key.replace('_', ' ').title(), f"{value:.1f}%")
                    else:
                        st.metric(key.replace('_', ' ').title(), f"{value:,}")
        except Exception:
            st.write("Could not load database statistics")


def main():
    st.sidebar.title("ðŸ“Š Stock Outlier Analysis")

    # Initialize session state for slider values
    if 'reset_weights' not in st.session_state:
        st.session_state.reset_weights = False

    # Default weights (40/25/20/15)
    default_fund = 0.4
    default_qual = 0.25
    default_growth = 0.2
    default_sent = 0.15

    # Check if reset was requested
    if st.session_state.reset_weights:
        st.session_state.reset_weights = False
        # Clear any existing slider values from session state
        for key in list(st.session_state.keys()):
            if key.startswith('slider_'):
                del st.session_state[key]

    # Sidebar for weight adjustment (below navigation)
    with st.sidebar:
        st.markdown("---")
        st.header("ðŸŽ›ï¸ Adjust Component Weights")
        st.markdown("Modify the weights to see how it affects stock rankings:")

        # Sliders with unique keys for session state control
        fund_weight = st.slider(
            "ðŸ¢ Fundamental",
            min_value=0.0,
            max_value=0.8,
            value=default_fund,
            step=0.05,
            help="P/E, EV/EBITDA, PEG, FCF Yield",
            key="slider_fund"
        )

        qual_weight = st.slider(
            "ðŸ’Ž Quality",
            min_value=0.0,
            max_value=0.6,
            value=default_qual,
            step=0.05,
            help="ROE, ROIC, Debt Ratios, Current Ratio",
            key="slider_qual"
        )

        growth_weight = st.slider(
            "ðŸ“ˆ Growth",
            min_value=0.0,
            max_value=0.5,
            value=default_growth,
            step=0.05,
            help="Revenue Growth, EPS Growth, Stability",
            key="slider_growth"
        )

        sent_weight = st.slider(
            "ðŸ’­ Sentiment",
            min_value=0.0,
            max_value=0.4,
            value=default_sent,
            step=0.05,
            help="News Sentiment, Social Media, Volume",
            key="slider_sent"
        )

        # Show normalized weights
        custom_weights = [fund_weight, qual_weight, growth_weight, sent_weight]
        total_weight = sum(custom_weights)
        normalized_weights = [w/total_weight for w in custom_weights]

        st.markdown("**Normalized Weights:**")
        st.write(f"â€¢ Fundamental: {normalized_weights[0]:.1%}")
        st.write(f"â€¢ Quality: {normalized_weights[1]:.1%}")
        st.write(f"â€¢ Growth: {normalized_weights[2]:.1%}")
        st.write(f"â€¢ Sentiment: {normalized_weights[3]:.1%}")

        # Reset button
        if st.button("ðŸ”„ Reset to Default (40/25/20/15)"):
            st.session_state.reset_weights = True
            st.rerun()

    # Header with logo and brand styling
    try:
        # Create inline header with logo matching font size
        header_html = '''
        <div style="display: flex; align-items: center; justify-content: center; margin-bottom: 1rem;">
            <img src="data:image/png;base64,{}" style="height: 2.5rem; margin-right: 15px;" />
            <h1 class="main-header" style="margin: 0;">Stock Outlier Analysis - S&P 500</h1>
        </div>
        '''

        # Read and encode the logo
        import base64
        with open("src/data/Logo-Element-Retina.png", "rb") as f:
            logo_data = base64.b64encode(f.read()).decode()

        st.markdown(header_html.format(logo_data), unsafe_allow_html=True)
    except:
        # Fallback if logo not found
        st.markdown('<h1 class="main-header">ðŸ“Š Stock Outlier Analysis - S&P 500</h1>', unsafe_allow_html=True)
    st.markdown("### Professional Stock Mispricing Detection Dashboard")

    # Load data
    with st.spinner("Loading stock analysis data..."):
        df = load_stock_data()
        stats = get_database_stats()

    if df.empty:
        st.error("No data available. Please check database connection.")
        return

    # Create tabs for different views
    tab1, tab2, tab3, tab4 = st.tabs(["ðŸ“ˆ Rankings", "ðŸ“Š Individual Analysis", "ðŸ—„ï¸ Data Management", "ðŸ“š Methodology"])

    with tab1:
        # Check if weights have been adjusted
        default_weights = [0.4, 0.25, 0.2, 0.15]
        weights_changed = not np.allclose(normalized_weights, default_weights, atol=0.01)

        # Summary metrics
        st.subheader("ðŸ“‹ Analysis Summary")
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric(
                "Stocks Analyzed",
                f"{stats['calculated_stocks']}/{stats['total_stocks']}",
                f"{stats['coverage_pct']:.1f}%"
            )

        with col2:
            st.metric(
                "Data Quality",
                "Enhanced",
                "v1.1 fallbacks"
            )

        with col3:
            last_calc_display = stats['last_calculation'][:10] if stats['last_calculation'] else "Unknown"
            st.metric(
                "Last Updated",
                last_calc_display,
                "Auto-refresh"
            )

        with col4:
            # Calculate average score
            avg_score = df['composite_score'].mean()
            st.metric(
                "Avg Score",
                f"{avg_score:.1f}",
                "Market baseline"
            )

        # Weight adjustment section
        if weights_changed:
            st.subheader("ðŸŽ›ï¸ Custom Weight Analysis")
            adjusted_df = calculate_custom_composite_scores(df, normalized_weights)
            show_weight_adjusted_rankings(df, adjusted_df, normalized_weights)
            # Use adjusted rankings for subsequent analysis
            main_df = adjusted_df
            score_column = 'custom_composite_score'
        else:
            main_df = df
            score_column = 'composite_score'

        # Top/Bottom performers
        st.subheader("ðŸ† Performance Leaders")
        col_under, col_over = st.columns(2)

        with col_under:
            show_top_performers(main_df, 5, False, score_column)

        with col_over:
            show_top_performers(main_df, 5, True, score_column)

        # Analytics visualizations
        st.subheader("ðŸ“Š Distribution Analysis")

        # Create enhanced histogram with stock ticker hover data
        # First, create bins and aggregate stock symbols per bin
        scores = main_df[score_column].values
        n_bins = 20
        bin_edges = np.linspace(scores.min(), scores.max(), n_bins + 1)

        # Assign each stock to a bin
        bin_indices = np.digitize(scores, bin_edges) - 1
        bin_indices = np.clip(bin_indices, 0, n_bins - 1)

        # Aggregate stocks by bin
        bin_data = []
        for i in range(n_bins):
            mask = bin_indices == i
            stocks_in_bin = main_df[mask]['symbol'].tolist()
            count = len(stocks_in_bin)
            bin_center = (bin_edges[i] + bin_edges[i + 1]) / 2
            bin_range = f"{bin_edges[i]:.1f}-{bin_edges[i+1]:.1f}"

            # Format stock list for hover (max 20 per line for readability)
            if len(stocks_in_bin) > 0:
                stock_lines = []
                for j in range(0, len(stocks_in_bin), 20):
                    stock_lines.append(', '.join(stocks_in_bin[j:j+20]))
                stocks_text = '<br>'.join(stock_lines)
            else:
                stocks_text = "None"

            bin_data.append({
                'bin_center': bin_center,
                'bin_range': bin_range,
                'count': count,
                'stocks': stocks_text
            })

        bin_df = pd.DataFrame(bin_data)

        # Create histogram using bar chart for better control
        fig_hist = go.Figure()
        fig_hist.add_trace(go.Bar(
            x=bin_df['bin_center'],
            y=bin_df['count'],
            width=(bin_edges[1] - bin_edges[0]) * 0.9,
            hovertemplate='<b>Score Range:</b> %{customdata[0]}<br>' +
                          '<b>Number of Stocks:</b> %{y}<br>' +
                          '<b>Tickers:</b><br>%{customdata[1]}<extra></extra>',
            customdata=bin_df[['bin_range', 'stocks']].values,
            marker_color='#636EFA'
        ))

        fig_hist.update_layout(
            title="Score Distribution",
            xaxis_title="Composite Score",
            yaxis_title="Number of Stocks",
            font=dict(family='Montserrat'),
            hovermode='closest'
        )

        # Create enhanced box plot with individual stock points and outlier identification
        # Calculate quartiles and IQR for outlier detection
        Q1 = main_df[score_column].quantile(0.25)
        Q3 = main_df[score_column].quantile(0.75)
        IQR = Q3 - Q1
        lower_fence = Q1 - 1.5 * IQR
        upper_fence = Q3 + 1.5 * IQR

        # Identify outliers
        main_df['is_outlier'] = (main_df[score_column] < lower_fence) | (main_df[score_column] > upper_fence)

        fig_box = go.Figure()

        # Add box plot without outliers shown
        fig_box.add_trace(go.Box(
            y=main_df[score_column],
            name="",
            boxpoints=False,  # We'll add points manually
            marker_color='#636EFA',
            showlegend=False
        ))

        # Add all data points with enhanced hover info
        # Outliers get different styling
        outliers = main_df[main_df['is_outlier']]
        non_outliers = main_df[~main_df['is_outlier']]

        # Add non-outlier points (smaller, more transparent)
        if len(non_outliers) > 0:
            fig_box.add_trace(go.Scatter(
                x=[0] * len(non_outliers),
                y=non_outliers[score_column],
                mode='markers',
                name='Normal Range',
                marker=dict(
                    size=6,
                    color='rgba(99, 110, 250, 0.3)',
                    line=dict(width=1, color='rgba(99, 110, 250, 0.5)')
                ),
                hovertemplate='<b>%{customdata[0]}</b><br>' +
                              'Score: %{y:.2f}<br>' +
                              'Company: %{customdata[1]}<extra></extra>',
                customdata=non_outliers[['symbol', 'company_name']].values,
                showlegend=True
            ))

        # Add outlier points (larger, highlighted)
        if len(outliers) > 0:
            fig_box.add_trace(go.Scatter(
                x=[0] * len(outliers),
                y=outliers[score_column],
                mode='markers',
                name='Outliers',
                marker=dict(
                    size=10,
                    color='#FF6B6B',
                    symbol='diamond',
                    line=dict(width=2, color='#C92A2A')
                ),
                hovertemplate='<b>%{customdata[0]}</b> (OUTLIER)<br>' +
                              'Score: %{y:.2f}<br>' +
                              'Company: %{customdata[1]}<extra></extra>',
                customdata=outliers[['symbol', 'company_name']].values,
                showlegend=True
            ))

        fig_box.update_layout(
            title="Score Distribution Box Plot",
            yaxis_title="Composite Score",
            xaxis=dict(showticklabels=False),
            font=dict(family='Montserrat'),
            hovermode='closest',
            showlegend=True,
            legend=dict(
                orientation="h",
                yanchor="bottom",
                y=1.02,
                xanchor="right",
                x=1
            )
        )

        chart_col1, chart_col2 = st.columns(2)

        with chart_col1:
            st.plotly_chart(fig_hist, use_container_width=True)

        with chart_col2:
            st.plotly_chart(fig_box, use_container_width=True)

        # Statistical summary
        st.subheader("ðŸ“ˆ Statistical Summary")
        summary_col1, summary_col2, summary_col3 = st.columns(3)

        # Use appropriate score column
        analysis_scores = main_df[score_column]

        with summary_col1:
            st.metric("Mean Score", f"{analysis_scores.mean():.1f}")
            st.metric("Std Deviation", f"{analysis_scores.std():.1f}")

        with summary_col2:
            st.metric("Median Score", f"{analysis_scores.median():.1f}")
            st.metric("Interquartile Range", f"{analysis_scores.quantile(0.75) - analysis_scores.quantile(0.25):.1f}")

        with summary_col3:
            st.metric("75th Percentile", f"{analysis_scores.quantile(0.75):.1f}")
            st.metric("25th Percentile", f"{analysis_scores.quantile(0.25):.1f}")

        # Sector analysis
        st.subheader("ðŸ­ Sector Performance")

        if 'sector' in main_df.columns:
            sector_stats = main_df.groupby('sector').agg({
                score_column: 'mean',
                'symbol': 'count',
                'fundamental_score': 'mean',
                'quality_score': 'mean',
                'growth_score': 'mean',
                'sentiment_score': 'mean'
            }).round(1)

            sector_stats.columns = ['Avg Composite', 'Stock Count', 'Avg Fund', 'Avg Quality', 'Avg Growth', 'Avg Sentiment']
            sector_stats = sector_stats.sort_values('Avg Composite', ascending=False)

            st.dataframe(sector_stats, use_container_width=True)

        # Footer
        st.markdown("---")
        st.markdown("""
        **Methodology**: 4-component weighted analysis (Fundamental 40%, Quality 25%, Growth 20%, Sentiment 15%)
        **Data Sources**: Yahoo Finance, Reddit API
        **Last Updated**: {}
        **âš ï¸ Disclaimer**: For educational purposes only. Not investment advice.
        """.format(stats['last_calculation'][:10] if stats['last_calculation'] else "Unknown"))

    with tab2:
        show_individual_stock_analysis(df)

    with tab3:
        show_data_management()

    with tab4:
        show_methodology_guide()

if __name__ == "__main__":
    main()